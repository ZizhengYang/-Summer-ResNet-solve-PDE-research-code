{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate as integrate\n",
    "import scipy.special as special\n",
    "\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim\n",
    "\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import exp,arange\n",
    "from pylab import meshgrid,cm,imshow,contour,clabel,colorbar,axis,title,show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sin(\\pi x) e^{-\\pi^2 t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the analytical representation of exact solution\n",
    "def PDE_analytical_solu(x, t):\n",
    "    return np.sin(np.pi * x) * np.exp(-np.power(np.pi, 2) * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========\n",
    "# padding method\n",
    "# ===========\n",
    "\n",
    "# padding and zero padding\n",
    "def padding(original, starting_padding, end_padding):\n",
    "    return np.hstack((starting_padding, original, end_padding)).tolist()\n",
    "\n",
    "def zero_padding(original, num):\n",
    "    zero_list = [0 for i in range(num)]\n",
    "    return padding(original, zero_list, zero_list)\n",
    "\n",
    "def border_padding(original, num):\n",
    "    starting = original[0]\n",
    "    ending = original[len(original)-1]\n",
    "    starting_padding = [starting for i in range(num)]\n",
    "    end_padding = [ending for i in range(num)]\n",
    "    return padding(original, starting_padding, end_padding)\n",
    "\n",
    "def recursive_padding(original, num):\n",
    "    starting_padding = original[-num:]\n",
    "    end_padding = original[:num]\n",
    "    return padding(original, starting_padding, end_padding)\n",
    "\n",
    "def random_padding(original, num):\n",
    "    max_value = np.max(original)\n",
    "    min_value = np.min(original)\n",
    "    random_list_1 = [random.randint(min_value, max_value) for i in range(num)]\n",
    "    random_list_2 = [random.randint(min_value, max_value) for i in range(num)]\n",
    "    return padding(original, random_list_1, random_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_analytical_solu(delta_x=1/20, delta_t=1/20, xmin=0, tmin=0, xmax=2 * np.pi, tmax=2 * np.pi, analytical_eq=PDE_analytical_solu):\n",
    "    x = arange(xmin, xmax, delta_x)\n",
    "    t = arange(tmin, tmax, delta_t)\n",
    "    X,T = meshgrid(x, t) # grid of point\n",
    "    solu = analytical_eq(X, T) # evaluation of the function on the grid\n",
    "    return x, t, solu\n",
    "\n",
    "def gen_discrete_average_solu(delta_x=1/20, delta_t=1/20, xmin=0, tmin=0, xmax=2 * np.pi, tmax=2 * np.pi, analytical_eq=PDE_analytical_solu):\n",
    "    x = arange(xmin, xmax, delta_x)\n",
    "    t = arange(tmin, tmax, delta_t)\n",
    "    X,T = meshgrid(x, t) # grid of point\n",
    "    Z = analytical_eq(X, T) # evaluation of the function on the grid\n",
    "    solu = []\n",
    "    for zz in Z:\n",
    "        solu_t = []\n",
    "        for j in range(len(zz)-1):\n",
    "            value = (1/2) * (zz[j] + zz[j+1])\n",
    "            solu_t.append(value)\n",
    "        solu.append(solu_t)\n",
    "    return x, t, solu\n",
    "\n",
    "def gen_cell_average_solu(delta_x=1/20, delta_t=1/20, xmin=0, tmin=0, xmax=2 * np.pi, tmax=2 * np.pi, analytical_eq=PDE_analytical_solu):\n",
    "    x = arange(xmin, xmax, delta_x)\n",
    "    t = arange(tmin, tmax, delta_t)\n",
    "    solu = []\n",
    "    for ti in range(len(t)):\n",
    "        solu_t = []\n",
    "        for j in range(len(x)-1):\n",
    "            value = integrate.quad(lambda x: analytical_eq(x, t[ti]), x[j], x[j+1])\n",
    "            value = value[0] * (1/delta_x)\n",
    "            solu_t.append(value)\n",
    "        solu.append(solu_t)\n",
    "    return x, t, solu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========\n",
    "# training set\n",
    "# ===========\n",
    "\n",
    "def get_trainingset_random(solu, num=3, padding=border_padding, size=10000):\n",
    "    solu_padding = []\n",
    "    pairs = []\n",
    "    for item in solu:\n",
    "        p = padding(item, num)\n",
    "        solu_padding.append(p)\n",
    "    for iteration in range(num):\n",
    "        t_index = random.randint(0, len(solu_padding)-2)\n",
    "        time = solu_padding[t_index]\n",
    "        time_next = solu_padding[t_index+1]\n",
    "        x_index = random.randint(0, len(p)-2*num-1)\n",
    "        train = p[x_index:x_index+2*num+1]\n",
    "        target = p_next[x_index+num]\n",
    "        pair = {'train': train, 'target': target}\n",
    "        pairs.append(pair)\n",
    "    return pairs\n",
    "    \n",
    "def get_trainingset_all(solu, num=3, padding=border_padding):\n",
    "    solu_cutted = solu[:-1]\n",
    "    pairs = []\n",
    "    for index, item in enumerate(solu_cutted):\n",
    "        p = padding(item, num)\n",
    "        p_next = padding(solu[index+1], num)\n",
    "        for xi in range(len(p)-2*num-1):\n",
    "            train = p[xi:xi+2*num+1]\n",
    "            target = p_next[xi+num]\n",
    "            pair = {'train': train, 'target': target}\n",
    "            pairs.append(pair)\n",
    "    return pairs\n",
    "\n",
    "# ===========\n",
    "#  testing set\n",
    "# ===========\n",
    "\n",
    "def get_testingset_random(solu, num=3, padding=border_padding, size=10000):\n",
    "    return get_trainingset_random(solu, num=3, padding=border_padding, size=10000)\n",
    "\n",
    "def get_testingset_all(solu, num=3, padding=border_padding):\n",
    "    solu_cutted = solu[:-1]\n",
    "    pairs = []\n",
    "    for index, item in enumerate(solu_cutted):\n",
    "        p = padding(item, num)\n",
    "        p_next = padding(solu[index+1], num)\n",
    "        pairs_t = []\n",
    "        for xi in range(len(p)-2*num-1):\n",
    "            train = p[xi:xi+2*num+1]\n",
    "            target = p_next[xi+num]\n",
    "            pair = {'train': train, 'target': target}\n",
    "            pairs_t.append(pair)\n",
    "        pairs.append(pairs_t)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============\n",
    "# FullconnectedResNet\n",
    "# ==============\n",
    "\n",
    "class FullconnectedResNet(nn.Module):\n",
    "    def __init__(self, i, o, layer_data):\n",
    "        super(FullconnectedResNet, self).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module(\"linear_1\", nn.Linear(i, layer_data[0]))\n",
    "        self.layers.add_module(\"dropout_1\", nn.Dropout(p=0.2))\n",
    "        self.layers.add_module(\"relu_1\", nn.ReLU())\n",
    "        for index in range(len(layer_data)-1):\n",
    "            self.layers.add_module(\"linear_\"+str(index+2), nn.Linear(layer_data[index], layer_data[index+1]))\n",
    "            self.layers.add_module(\"dropout_2\", nn.Dropout(p=0.2))\n",
    "            self.layers.add_module(\"relu_\"+str(index+2), nn.ReLU())\n",
    "        self.layers.add_module(\"linear_3\"+str(len(layer_data)+1), nn.Linear(layer_data[len(layer_data)-1], o))\n",
    "        self.layers.add_module(\"dropout_3\", nn.Dropout(p=0.2))\n",
    "        self.layers.add_module(\"relu_\"+str(len(layer_data)+1), nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "        return output + x[3]\n",
    "\n",
    "    def load_model(self, save_path):\n",
    "        self.load_state_dict(torch.load(save_path))\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "        torch.save(self.state_dict(), save_path)\n",
    "        \n",
    "# ==========\n",
    "# BidirectionRNN\n",
    "# ==========\n",
    "\n",
    "# class BidirectionRNN(nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.3017], grad_fn=<AddBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1021, -0.1698,  0.0429,  0.0598,  0.3485, -0.2723,  0.3015],\n",
      "        [ 0.3165, -0.0181,  0.1769, -0.2968, -0.0663, -0.0700, -0.1977],\n",
      "        [-0.1738, -0.0875, -0.2608,  0.0181,  0.0281, -0.2068,  0.0400],\n",
      "        [-0.2062,  0.1331,  0.3116, -0.1868,  0.0567,  0.2738, -0.3637],\n",
      "        [ 0.2782, -0.1856,  0.2958,  0.1318, -0.2266, -0.2164,  0.2181],\n",
      "        [-0.3492,  0.1270,  0.1825, -0.0359, -0.0840,  0.2736,  0.0127]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1652, -0.2969,  0.3083, -0.1386,  0.3694, -0.2173],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3451,  0.0913, -0.2832,  0.0801,  0.2072,  0.2093],\n",
      "        [-0.3015, -0.2657,  0.1951,  0.0368, -0.0504, -0.2825],\n",
      "        [-0.0697, -0.0164,  0.3995,  0.0483, -0.0648,  0.0101],\n",
      "        [-0.3984, -0.3061,  0.3137,  0.1154,  0.0788, -0.2893],\n",
      "        [ 0.0030, -0.0116,  0.3214,  0.2207, -0.1216, -0.1457],\n",
      "        [-0.3800,  0.1861, -0.3172,  0.2121, -0.3982,  0.3581]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3564, -0.2730, -0.0369, -0.1564, -0.3807,  0.2143],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1470, -0.0444, -0.2443, -0.2643, -0.3126,  0.3693],\n",
      "        [-0.3473,  0.2523,  0.2200, -0.3547, -0.3668, -0.0305],\n",
      "        [ 0.2265, -0.0016, -0.2632,  0.0543, -0.0191, -0.3059],\n",
      "        [-0.2211, -0.3592,  0.0059,  0.1244,  0.0440, -0.1560],\n",
      "        [-0.4055, -0.2633,  0.1636,  0.0193,  0.3640,  0.0370],\n",
      "        [-0.3437,  0.2273,  0.2032,  0.2538,  0.3326,  0.0891]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0042, -0.3064, -0.0640, -0.0793, -0.3359, -0.2703],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0568, -0.2420,  0.2708, -0.1845,  0.0216, -0.1954]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2411], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = FullconnectedResNet(i=7, o=1, layer_data=[6, 6, 6])\n",
    "out = model(torch.FloatTensor([1,2,3,4,5,6,7]))\n",
    "print(out)\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============\n",
    "# save and load: json\n",
    "# ============\n",
    "\n",
    "def save_json(save_path, data):\n",
    "    assert save_path.split('.')[-1] == 'json'\n",
    "    with open(save_path,'w+') as file:\n",
    "        json.dump(data,file)\n",
    "\n",
    "def load_json(file_path):\n",
    "    assert file_path.split('.')[-1] == 'json'\n",
    "    with open(file_path,'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# ============\n",
    "# save and load: csv\n",
    "# ============\n",
    "\n",
    "def save_csv(save_path, data):\n",
    "    with open(save_path, \"w+\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)\n",
    "\n",
    "# def load_csv(file_path):\n",
    "    \n",
    "# ==============\n",
    "# save and load: normal\n",
    "# ==============\n",
    "\n",
    "def save_list(save_path, data):\n",
    "    file = open(save_path, 'w+')\n",
    "    for value in data:\n",
    "        file.write(str(value)+\" \")\n",
    "    file.close()\n",
    "\n",
    "def load_list(file_path):\n",
    "    string = []\n",
    "    with open(file_path, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i%2==0:\n",
    "            string.append(row)       \n",
    "    the_list = []\n",
    "    for p in string:\n",
    "        the_list.append(np.array([float(i) for i in p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_the_model_FullconnectedResNet(delta_x=1/20 * np.pi, delta_t=1/20 * np.pi, xmin=0, tmin=0, xmax=2 * np.pi, tmax=2 * np.pi, analytical_eq=PDE_analytical_solu, gen_solution=gen_cell_average_solu,\n",
    "                       allTheTime=False,\n",
    "                       num=3, padding=recursive_padding,\n",
    "                       i=7, o=1, layer_data=[6, 6],\n",
    "                       lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False,\n",
    "                       iteration=2,\n",
    "                       json_file=\"config.json\", solu_file=\"solu.csv\", input_file=\"inputs.txt\", loss_file=\"train_losses\", outout_file=\"outputs.txt\", model_file = \"model\"):\n",
    "    # ===============\n",
    "    # Prepare the training set\n",
    "    # ===============\n",
    "    x, t, solu = gen_solution(delta_x=delta_x, delta_t=delta_t, xmin=xmin, tmin=tmin, xmax=xmax, tmax=tmax, analytical_eq=PDE_analytical_solu)\n",
    "    actual_solu = solu\n",
    "    if not allTheTime:\n",
    "        solu = solu[:2]\n",
    "    pairs = get_trainingset_all(solu, num=num, padding=padding)\n",
    "    # ==============\n",
    "    # Set the saving pathes\n",
    "    # ==============\n",
    "    f = open(\"counter.txt\")\n",
    "    list_of_counters = []\n",
    "    for line in open(\"counter.txt\"):\n",
    "        list_of_counters.append(line)\n",
    "    experiment_counter = int(list_of_counters[0])\n",
    "    folder_name = \"experiment-\" + list_of_counters[0]\n",
    "    experiment_counter += 1\n",
    "    with open(\"counter.txt\",\"w\") as f:\n",
    "        f.write(str(experiment_counter))\n",
    "    folder = os.path.exists(folder_name)\n",
    "    if not folder:\n",
    "        os.makedirs(folder_name)\n",
    "    json_file = folder_name+\"/\"+json_file\n",
    "    solu_file_used = folder_name+\"/\"+\"u_ \"+solu_file\n",
    "    solu_file_actual = folder_name+\"/\"+\"a_ \"+solu_file\n",
    "    input_file = folder_name+\"/\"+input_file\n",
    "    loss_file_txt = folder_name+\"/\"+loss_file + \".txt\"\n",
    "    loss_file_csv = folder_name+\"/\"+loss_file + \".csv\"\n",
    "    outout_file = folder_name+\"/\"+outout_file\n",
    "    model_file = folder_name+\"/\"+model_file\n",
    "    # =================\n",
    "    # Set up model & optimizer\n",
    "    # =================\n",
    "    model = FullconnectedResNet(i=i, o=o, layer_data=layer_data)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "    criterion = nn.MSELoss()\n",
    "    # ===========\n",
    "    # Train the model\n",
    "    # ===========\n",
    "    model.train()\n",
    "    list_of_loss = []\n",
    "    list_of_output = []\n",
    "    counter = 0\n",
    "    for itera in range(iteration):\n",
    "        for pair in pairs:\n",
    "            output = model(torch.FloatTensor(pair[\"train\"]))\n",
    "            loss = criterion(output, torch.FloatTensor([pair[\"target\"]]))\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            list_of_loss.append(loss.item())\n",
    "            list_of_output.append(output)\n",
    "            # print(\"=\"*40)\n",
    "            # print(\"The\", counter, \"time of training\")\n",
    "            # print(\"We are using:\", pair[\"train\"])\n",
    "           # print(\"It should have be:\", pair[\"target\"])\n",
    "            # print(\"But prediction is:\", output)\n",
    "            # print(\"The training loss is:\", loss.item())\n",
    "            counter += 1\n",
    "    # ============\n",
    "    # Save the json data\n",
    "    # ============\n",
    "    data_setting = {'type': 'data', 'padding': str(border_padding), 'data_getter': str(get_trainingset_all)}\n",
    "    range_setting = {'type': 'range', 'delta_x': delta_x,'delta_t': delta_t,'xmin': xmin, 'tmin': tmin, 'xmax': xmax, 'tmax': tmax}\n",
    "    optimizer_setting = {'type': 'optimizer', 'optimizer': str(optim.Adam), 'learning_rate': lr, 'betas_values': betas, 'eps': eps, 'weight_decay': weight_decay, 'amsgrad': amsgrad}\n",
    "    model_setting = {'type': 'model', 'model': 'full connected ResNet', 'input_dimension': i, 'output_dimension': o, 'hidden_dimensions': layer_data}\n",
    "    differential_eq_setting = {'type': 'equation', 'analytical_eq': str(analytical_eq), 'range_setting': range_setting}\n",
    "    training_setting = {'type': 'training', 'iteration': iteration, 'range_setting': range_setting, 'model_setting': model_setting, 'optimizer_setting': optimizer_setting}\n",
    "    json_data = {'differential_eq_setting': differential_eq_setting, 'training_setting': training_setting}\n",
    "    save_json(json_file, json_data)\n",
    "    # ========================\n",
    "    # Save the calculated analytical solution\n",
    "    # ========================\n",
    "    save_csv(solu_file_used, solu)\n",
    "    save_csv(solu_file_actual, actual_solu)\n",
    "    # ====================\n",
    "    # Save the inputs/outputs/losses\n",
    "    # ====================\n",
    "    save_list(input_file, pairs)\n",
    "    save_list(outout_file, list_of_output)\n",
    "    save_list(loss_file_txt, list_of_loss)\n",
    "    save_csv(loss_file_csv, [list_of_loss])\n",
    "    # ==========\n",
    "    # Save the model\n",
    "    # ==========\n",
    "    model.save_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_the_model_FullconnectedResNet(delta_x=1/20 * np.pi, delta_t=1/20 * np.pi, xmin=0, tmin=0, xmax=2 * np.pi, tmax=2 * np.pi, analytical_eq=PDE_analytical_solu, gen_solution=gen_cell_average_solu,\n",
    "                       num=3, padding=recursive_padding,\n",
    "                       i=7, o=1, layer_data=[6, 6],\n",
    "                       experiment_id = 1,\n",
    "                       model_file = \"model\", loss_file=\"eval_losses.txt\", err_file=\"errs.txt\", predict_file=\"prediction.csv\"):\n",
    "    # ===============\n",
    "    # Prepare the training set\n",
    "    # ===============\n",
    "    x, t, solu = gen_solution(delta_x=delta_x, delta_t=delta_t, xmin=xmin, tmin=tmin, xmax=xmax, tmax=tmax, analytical_eq=PDE_analytical_solu)\n",
    "    pairs = get_testingset_all(solu, num=num, padding=padding)\n",
    "    # ==============\n",
    "    # Set the saving pathes\n",
    "    # ==============\n",
    "    folder_name = \"experiment-\" + str(experiment_id)\n",
    "    model_file=folder_name+\"/\"+model_file\n",
    "    loss_file=folder_name+\"/\"+loss_file\n",
    "    err_file=folder_name+\"/\"+err_file\n",
    "    predict_file=folder_name+\"/\"+predict_file\n",
    "    # print(folder_name)\n",
    "    # print(model_file)\n",
    "    # print(loss_file)\n",
    "    # print(err_file)\n",
    "    # print(predict_file)\n",
    "    # =================\n",
    "    # Set up model & optimizer\n",
    "    # =================\n",
    "    model = FullconnectedResNet(i=i, o=o, layer_data=layer_data)\n",
    "    model.load_model(model_file)\n",
    "    criterion = nn.MSELoss()\n",
    "    # ===========\n",
    "    # Train the model\n",
    "    # ===========\n",
    "    model.eval()\n",
    "    list_of_loss = []\n",
    "    list_of_error = []\n",
    "    model_result = []\n",
    "    counter = 0\n",
    "    model_result.append(solu[0])\n",
    "    for j in range(len(pairs)):\n",
    "        pairs_t = pairs[j]\n",
    "        model_result_t = []\n",
    "        for pair in pairs_t:\n",
    "            output = model(torch.FloatTensor(pair[\"train\"]))\n",
    "            loss = criterion(output, torch.FloatTensor([pair[\"target\"]]))\n",
    "            error = str(np.absolute(output.item() - pair[\"target\"]) / pair[\"target\"])\n",
    "            # if counter % 1000 == 0:\n",
    "            #     print(\"=\"*40)\n",
    "            #     print(\"The\", counter, \"time of training\")\n",
    "            #     print(\"The time segement is:\", j)\n",
    "            #     print(\"The input pair is:\", pair[\"train\"])\n",
    "            #     print(\"It should have be:\", pair[\"target\"])\n",
    "            #     print(\"The model prediction is:\", output)\n",
    "            #     print(\"The evaluation loss is:\", loss)\n",
    "            #     print(\"The error in percentage is:\", error)\n",
    "            model_result_t.append(output.item())\n",
    "            list_of_loss.append(loss.item())\n",
    "            list_of_error.append(error)\n",
    "            counter += 1\n",
    "        model_result.append(model_result_t)\n",
    "    # =================\n",
    "    # Save the losses and error\n",
    "    # =================\n",
    "    save_list(loss_file, list_of_loss)\n",
    "    save_list(err_file, list_of_error)\n",
    "    # =============\n",
    "    # Save the prediction\n",
    "    # =============\n",
    "    save_csv(predict_file, model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_the_model_FullconnectedResNet(delta_x=1/20, delta_t=1/10, xmin=0, tmin=0, xmax=2, tmax=1, analytical_eq=PDE_analytical_solu, gen_solution=gen_cell_average_solu,\n",
    "                       allTheTime=False,\n",
    "                       num=3, padding=recursive_padding,\n",
    "                       i=7, o=1, layer_data=[6, 6],\n",
    "                       lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False,\n",
    "                       iteration=1000,\n",
    "                       json_file=\"config.json\", solu_file=\"solu.csv\", input_file=\"inputs.txt\", loss_file=\"train_losses\", outout_file=\"outputs.txt\", model_file = \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testing_the_model_FullconnectedResNet(delta_x=1/20, delta_t=1/10, xmin=0, tmin=0, xmax=2, tmax=1, analytical_eq=PDE_analytical_solu, gen_solution=gen_cell_average_solu,\n",
    "                       num=3, padding=recursive_padding,\n",
    "                       i=7, o=1, layer_data=[6, 6],\n",
    "                       experiment_id = 2,\n",
    "                       model_file = \"model\", loss_file=\"eval_losses.txt\", err_file=\"errs.txt\", predict_file=\"prediction.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
