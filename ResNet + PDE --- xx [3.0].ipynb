{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate as integrate\n",
    "import scipy.special as special\n",
    "\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim\n",
    "\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import exp,arange\n",
    "from pylab import meshgrid,cm,imshow,contour,clabel,colorbar,axis,title,show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sin(\\pi x) e^{-\\pi^2 t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_equ_analytical_solu(x, t):\n",
    "    return np.sin(np.pi * x) * np.exp(-np.power(np.pi, 2) * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============\n",
    "# save and load: json\n",
    "# ============\n",
    "\n",
    "def save_json(save_path, data):\n",
    "    assert save_path.split('.')[-1] == 'json'\n",
    "    with open(save_path,'w+') as file:\n",
    "        json.dump(data,file)\n",
    "\n",
    "def load_json(file_path):\n",
    "    assert file_path.split('.')[-1] == 'json'\n",
    "    with open(file_path,'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# ============\n",
    "# save and load: csv\n",
    "# ============\n",
    "\n",
    "def save_csv(save_path, data):\n",
    "    with open(save_path, \"w+\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)\n",
    "\n",
    "def load_csv(file_path):\n",
    "    string = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            if i%2==0:\n",
    "                string.append(row)\n",
    "    ret = []\n",
    "    for p in string:\n",
    "        ret.append(np.array([float(i) for i in p]))\n",
    "    return ret\n",
    "\n",
    "# ==============\n",
    "# save and load: normal\n",
    "# ==============\n",
    "\n",
    "def save_list(save_path, data):\n",
    "    file = open(save_path, 'w+')\n",
    "    for value in data:\n",
    "        file.write(str(value)+\" \")\n",
    "    file.close()\n",
    "\n",
    "# def load_list(file_path):\n",
    "\n",
    "# ==========\n",
    "# make directory\n",
    "# ==========\n",
    "\n",
    "def mkdir(folder_name):\n",
    "    folder = os.path.exists(folder_name)\n",
    "    if not folder:\n",
    "        os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======\n",
    "# OPTIONS\n",
    "# =======\n",
    "\n",
    "# ===========\n",
    "# padding method\n",
    "# ===========\n",
    "\n",
    "def padding(starting_padding, original, end_padding):\n",
    "    return np.hstack((starting_padding, original, end_padding)).tolist()\n",
    "\n",
    "def zero_padding(original, num1, num2):\n",
    "    starting_padding = [0 for i in range(num1)]\n",
    "    end_padding = [0 for i in range(num2)]\n",
    "    return padding(starting_padding, original, end_padding)\n",
    "\n",
    "def border_padding(original, num1, num2):\n",
    "    starting = original[0]\n",
    "    ending = original[len(original)-1]\n",
    "    starting_padding = [starting for i in range(num1)]\n",
    "    end_padding = [ending for i in range(num2)]\n",
    "    return padding(starting_padding, original, end_padding)\n",
    "\n",
    "def recursive_padding(original, num1, num2):\n",
    "    starting_padding = original[-num1:]\n",
    "    end_padding = original[:num2]\n",
    "    return padding(starting_padding, original, end_padding)\n",
    "\n",
    "def random_padding(original, num1, num2):\n",
    "    max_value = np.max(original)\n",
    "    min_value = np.min(original)\n",
    "    starting_padding = [random.randint(min_value, max_value) for i in range(num1)]\n",
    "    end_padding = [random.randint(min_value, max_value) for i in range(num2)]\n",
    "    return padding(starting_padding, original, end_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3,4,5,6,7,8,9,10]\n",
    "recursive_padding(a, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======\n",
    "# OPTIONS\n",
    "# =======\n",
    "\n",
    "# ==================\n",
    "# analytical solution generator\n",
    "# ==================\n",
    "\n",
    "def gen_analytical(delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq):\n",
    "    x = arange(xmin, xmax+delta_x, delta_x)\n",
    "    t = arange(tmin, tmax+delta_t, delta_t)\n",
    "    X,T = meshgrid(x, t) # grid of point\n",
    "    solu = analytical_eq(X, T) # evaluation of the function on the grid\n",
    "    return x, t, solu\n",
    "\n",
    "def gen_analytical_averaged(delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq):\n",
    "    x = arange(xmin, xmax+delta_x, delta_x)\n",
    "    t = arange(tmin, tmax+delta_t, delta_t)\n",
    "    X,T = meshgrid(x, t) # grid of point\n",
    "    Z = analytical_eq(X, T) # evaluation of the function on the grid\n",
    "    solu = []\n",
    "    for zz in Z:\n",
    "        solu_t = []\n",
    "        for j in range(len(zz)-1):\n",
    "            value = (1/2) * (zz[j] + zz[j+1])\n",
    "            solu_t.append(value)\n",
    "        solu.append(solu_t)\n",
    "    return x, t, solu\n",
    "\n",
    "def gen_analytical_cell_averaged(delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq):\n",
    "    x = arange(xmin, xmax+delta_x, delta_x)\n",
    "    t = arange(tmin, tmax+delta_t, delta_t)\n",
    "    solu = []\n",
    "    for ti in range(len(t)):\n",
    "        solu_t = []\n",
    "        for j in range(len(x)-1):\n",
    "            value = integrate.quad(lambda x: analytical_eq(x, t[ti]), x[j], x[j+1])\n",
    "            value = value[0] * (1/delta_x)\n",
    "            solu_t.append(value)\n",
    "        solu.append(solu_t)\n",
    "    return x, t, solu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======\n",
    "# OPTIONS\n",
    "# =======\n",
    "\n",
    "# ===========\n",
    "# training set\n",
    "# ===========\n",
    "\n",
    "def get_trainingset_random(solu, num1, num2, padding, size):\n",
    "    solu_padding = []\n",
    "    pairs = []\n",
    "    for item in solu:\n",
    "        p = padding(item, num1, num2)\n",
    "        solu_padding.append(p)\n",
    "    for iteration in range(size):\n",
    "        t_index = random.randint(0, len(solu_padding)-2)\n",
    "        x_index = random.randint(0, len(p)-num1-num2-1)\n",
    "        time = solu_padding[t_index]\n",
    "        time_next = solu_padding[t_index+1]\n",
    "        train = time[x_index: x_index+num1+num2+1]\n",
    "        target = time_next[x_index+num1]\n",
    "        pair = {'train': train, 'target': target}\n",
    "        pairs.append(pair)\n",
    "    return pairs\n",
    "    \n",
    "def get_trainingset_all(solu, num1, num2, padding):\n",
    "    solu_cutted = solu[:-1]\n",
    "    pairs = []\n",
    "    for index, item in enumerate(solu_cutted):\n",
    "        p = padding(item, num1, num2)\n",
    "        p_next = padding(solu[index+1], num1, num2)\n",
    "        for xi in range(len(p)-num1-num2):\n",
    "            train = p[xi: xi+num1+num2+1]\n",
    "            target = p_next[xi+num1]\n",
    "            pair = {'train': train, 'target': target}\n",
    "            pairs.append(pair)\n",
    "    return pairs\n",
    "\n",
    "# ===========\n",
    "#  testing set\n",
    "# ===========\n",
    "\n",
    "def get_testingset_random(solu, num1, num2, padding, size):\n",
    "    return get_trainingset_random(solu, num1, num2, padding, size)\n",
    "\n",
    "def get_testingset_all(solu, num1, num2, padding):\n",
    "    solu_cutted = solu[:-1]\n",
    "    pairs = []\n",
    "    for index, item in enumerate(solu_cutted):\n",
    "        p = padding(item, num1, num2)\n",
    "        p_next = padding(solu[index+1], num1, num2)\n",
    "        pairs_t = []\n",
    "        for xi in range(len(p)-num1-num2):\n",
    "            train = p[xi:xi+num1+num2+1]\n",
    "            target = p_next[xi+num1]\n",
    "            pair = {'train': train, 'target': target}\n",
    "            pairs_t.append(pair)\n",
    "        pairs.append(pairs_t)\n",
    "    return pairs\n",
    "\n",
    "# x, t, solu = gen_analytical_cell_averaged(delta_x=1/10, delta_t=1/20, xmin=0, tmin=0, xmax=1, tmax=2, analytical_eq=heat_equ_analytical_solu)\n",
    "# pairs = get_testingset_all(solu, 3, 3, recursive_padding)\n",
    "# for i, p in enumerate(pairs[0]):\n",
    "#     print(i, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15579194727527879, 0.45212584056020866, 0.7042025064251416, 0.8873469244938119, 0.9836316430834657, 0.9836316430834668, 0.8873469244938119, 0.7042025064251412, 0.45212584056020827, 0.155791947275279, -0.15579194727527892, -0.45212584056020916, -0.7042025064251403, -0.8873469244938128, -0.9836316430834647, -0.983631643083467, -0.8873469244938128, -0.7042025064251405, -0.45212584056020905, -0.15579194727527823]\n",
      "0 {'train': [-0.7042025064251405, -0.45212584056020905, -0.15579194727527823, 0.15579194727527879, 0.45212584056020866, 0.7042025064251416, 0.8873469244938119], 'target': 0.09511067616387091}\n",
      "1 {'train': [-0.45212584056020905, -0.15579194727527823, 0.15579194727527879, 0.45212584056020866, 0.7042025064251416, 0.8873469244938119, 0.9836316430834657], 'target': 0.27602193283364607}\n",
      "2 {'train': [-0.15579194727527823, 0.15579194727527879, 0.45212584056020866, 0.7042025064251416, 0.8873469244938119, 0.9836316430834657, 0.9836316430834668], 'target': 0.42991423955977376}\n",
      "3 {'train': [0.15579194727527879, 0.45212584056020866, 0.7042025064251416, 0.8873469244938119, 0.9836316430834657, 0.9836316430834668, 0.8873469244938119], 'target': 0.5417235451291507}\n",
      "4 {'train': [0.45212584056020866, 0.7042025064251416, 0.8873469244938119, 0.9836316430834657, 0.9836316430834668, 0.8873469244938119, 0.7042025064251412], 'target': 0.6005051756914073}\n",
      "5 {'train': [0.7042025064251416, 0.8873469244938119, 0.9836316430834657, 0.9836316430834668, 0.8873469244938119, 0.7042025064251412, 0.45212584056020827], 'target': 0.600505175691408}\n",
      "6 {'train': [0.8873469244938119, 0.9836316430834657, 0.9836316430834668, 0.8873469244938119, 0.7042025064251412, 0.45212584056020827, 0.155791947275279], 'target': 0.5417235451291506}\n",
      "7 {'train': [0.9836316430834657, 0.9836316430834668, 0.8873469244938119, 0.7042025064251412, 0.45212584056020827, 0.155791947275279, -0.15579194727527892], 'target': 0.42991423955977354}\n",
      "8 {'train': [0.9836316430834668, 0.8873469244938119, 0.7042025064251412, 0.45212584056020827, 0.155791947275279, -0.15579194727527892, -0.45212584056020916], 'target': 0.27602193283364584}\n",
      "9 {'train': [0.8873469244938119, 0.7042025064251412, 0.45212584056020827, 0.155791947275279, -0.15579194727527892, -0.45212584056020916, -0.7042025064251403], 'target': 0.09511067616387103}\n",
      "10 {'train': [0.7042025064251412, 0.45212584056020827, 0.155791947275279, -0.15579194727527892, -0.45212584056020916, -0.7042025064251403, -0.8873469244938128], 'target': -0.09511067616387099}\n",
      "11 {'train': [0.45212584056020827, 0.155791947275279, -0.15579194727527892, -0.45212584056020916, -0.7042025064251403, -0.8873469244938128, -0.9836316430834647], 'target': -0.2760219328336464}\n",
      "12 {'train': [0.155791947275279, -0.15579194727527892, -0.45212584056020916, -0.7042025064251403, -0.8873469244938128, -0.9836316430834647, -0.983631643083467], 'target': -0.42991423955977304}\n",
      "13 {'train': [-0.15579194727527892, -0.45212584056020916, -0.7042025064251403, -0.8873469244938128, -0.9836316430834647, -0.983631643083467, -0.8873469244938128], 'target': -0.5417235451291512}\n",
      "14 {'train': [-0.45212584056020916, -0.7042025064251403, -0.8873469244938128, -0.9836316430834647, -0.983631643083467, -0.8873469244938128, -0.7042025064251405], 'target': -0.6005051756914067}\n",
      "15 {'train': [-0.7042025064251403, -0.8873469244938128, -0.9836316430834647, -0.983631643083467, -0.8873469244938128, -0.7042025064251405, -0.45212584056020905], 'target': -0.600505175691408}\n",
      "16 {'train': [-0.8873469244938128, -0.9836316430834647, -0.983631643083467, -0.8873469244938128, -0.7042025064251405, -0.45212584056020905, -0.15579194727527823], 'target': -0.5417235451291511}\n",
      "17 {'train': [-0.9836316430834647, -0.983631643083467, -0.8873469244938128, -0.7042025064251405, -0.45212584056020905, -0.15579194727527823, 0.15579194727527879], 'target': -0.42991423955977304}\n",
      "18 {'train': [-0.983631643083467, -0.8873469244938128, -0.7042025064251405, -0.45212584056020905, -0.15579194727527823, 0.15579194727527879, 0.45212584056020866], 'target': -0.27602193283364623}\n",
      "19 {'train': [-0.8873469244938128, -0.7042025064251405, -0.45212584056020905, -0.15579194727527823, 0.15579194727527879, 0.45212584056020866, 0.7042025064251416], 'target': -0.09511067616387056}\n",
      "20 {'train': [-0.42991423955977304, -0.27602193283364623, -0.09511067616387056, 0.09511067616387091, 0.27602193283364607, 0.42991423955977376, 0.5417235451291507], 'target': 0.0580648799797379}\n",
      "21 {'train': [-0.27602193283364623, -0.09511067616387056, 0.09511067616387091, 0.27602193283364607, 0.42991423955977376, 0.5417235451291507, 0.6005051756914073], 'target': 0.16851084492498944}\n",
      "22 {'train': [-0.09511067616387056, 0.09511067616387091, 0.27602193283364607, 0.42991423955977376, 0.5417235451291507, 0.6005051756914073, 0.600505175691408], 'target': 0.26246179428488875}\n",
      "23 {'train': [0.09511067616387091, 0.27602193283364607, 0.42991423955977376, 0.5417235451291507, 0.6005051756914073, 0.600505175691408, 0.5417235451291506], 'target': 0.33072115454133344}\n",
      "24 {'train': [0.27602193283364607, 0.42991423955977376, 0.5417235451291507, 0.6005051756914073, 0.600505175691408, 0.5417235451291506, 0.42991423955977354], 'target': 0.3666072239214947}\n",
      "25 {'train': [0.42991423955977376, 0.5417235451291507, 0.6005051756914073, 0.600505175691408, 0.5417235451291506, 0.42991423955977354, 0.27602193283364584], 'target': 0.36660722392149514}\n",
      "26 {'train': [0.5417235451291507, 0.6005051756914073, 0.600505175691408, 0.5417235451291506, 0.42991423955977354, 0.27602193283364584, 0.09511067616387103], 'target': 0.33072115454133333}\n",
      "27 {'train': [0.6005051756914073, 0.600505175691408, 0.5417235451291506, 0.42991423955977354, 0.27602193283364584, 0.09511067616387103, -0.09511067616387099], 'target': 0.2624617942848886}\n",
      "28 {'train': [0.600505175691408, 0.5417235451291506, 0.42991423955977354, 0.27602193283364584, 0.09511067616387103, -0.09511067616387099, -0.2760219328336464], 'target': 0.1685108449249893}\n",
      "29 {'train': [0.5417235451291506, 0.42991423955977354, 0.27602193283364584, 0.09511067616387103, -0.09511067616387099, -0.2760219328336464, -0.42991423955977304], 'target': 0.05806487997973799}\n",
      "30 {'train': [0.42991423955977354, 0.27602193283364584, 0.09511067616387103, -0.09511067616387099, -0.2760219328336464, -0.42991423955977304, -0.5417235451291512], 'target': -0.058064879979737964}\n",
      "31 {'train': [0.27602193283364584, 0.09511067616387103, -0.09511067616387099, -0.2760219328336464, -0.42991423955977304, -0.5417235451291512, -0.6005051756914067], 'target': -0.16851084492498963}\n",
      "32 {'train': [0.09511067616387103, -0.09511067616387099, -0.2760219328336464, -0.42991423955977304, -0.5417235451291512, -0.6005051756914067, -0.600505175691408], 'target': -0.26246179428488825}\n",
      "33 {'train': [-0.09511067616387099, -0.2760219328336464, -0.42991423955977304, -0.5417235451291512, -0.6005051756914067, -0.600505175691408, -0.5417235451291511], 'target': -0.3307211545413338}\n",
      "34 {'train': [-0.2760219328336464, -0.42991423955977304, -0.5417235451291512, -0.6005051756914067, -0.600505175691408, -0.5417235451291511, -0.42991423955977304], 'target': -0.3666072239214943}\n",
      "35 {'train': [-0.42991423955977304, -0.5417235451291512, -0.6005051756914067, -0.600505175691408, -0.5417235451291511, -0.42991423955977304, -0.27602193283364623], 'target': -0.36660722392149514}\n",
      "36 {'train': [-0.5417235451291512, -0.6005051756914067, -0.600505175691408, -0.5417235451291511, -0.42991423955977304, -0.27602193283364623, -0.09511067616387056], 'target': -0.3307211545413338}\n",
      "37 {'train': [-0.6005051756914067, -0.600505175691408, -0.5417235451291511, -0.42991423955977304, -0.27602193283364623, -0.09511067616387056, 0.09511067616387091], 'target': -0.26246179428488836}\n",
      "38 {'train': [-0.600505175691408, -0.5417235451291511, -0.42991423955977304, -0.27602193283364623, -0.09511067616387056, 0.09511067616387091, 0.27602193283364607], 'target': -0.16851084492498952}\n",
      "39 {'train': [-0.5417235451291511, -0.42991423955977304, -0.27602193283364623, -0.09511067616387056, 0.09511067616387091, 0.27602193283364607, 0.42991423955977376], 'target': -0.058064879979737694}\n",
      "40 {'train': [-0.26246179428488836, -0.16851084492498952, -0.058064879979737694, 0.0580648799797379, 0.16851084492498944, 0.26246179428488875, 0.33072115454133344], 'target': 0.03544849456492551}\n",
      "41 {'train': [-0.16851084492498952, -0.058064879979737694, 0.0580648799797379, 0.16851084492498944, 0.26246179428488875, 0.33072115454133344, 0.3666072239214947], 'target': 0.10287553806257702}\n",
      "42 {'train': [-0.058064879979737694, 0.0580648799797379, 0.16851084492498944, 0.26246179428488875, 0.33072115454133344, 0.3666072239214947, 0.36660722392149514], 'target': 0.16023240711864242}\n",
      "43 {'train': [0.0580648799797379, 0.16851084492498944, 0.26246179428488875, 0.33072115454133344, 0.3666072239214947, 0.36660722392149514, 0.33072115454133333], 'target': 0.20190461176110852}\n",
      "44 {'train': [0.16851084492498944, 0.26246179428488875, 0.33072115454133344, 0.3666072239214947, 0.36660722392149514, 0.33072115454133333, 0.2624617942848886], 'target': 0.22381298625224844}\n",
      "45 {'train': [0.26246179428488875, 0.33072115454133344, 0.3666072239214947, 0.36660722392149514, 0.33072115454133333, 0.2624617942848886, 0.1685108449249893], 'target': 0.22381298625224869}\n",
      "46 {'train': [0.33072115454133344, 0.3666072239214947, 0.36660722392149514, 0.33072115454133333, 0.2624617942848886, 0.1685108449249893, 0.05806487997973799], 'target': 0.20190461176110852}\n",
      "47 {'train': [0.3666072239214947, 0.36660722392149514, 0.33072115454133333, 0.2624617942848886, 0.1685108449249893, 0.05806487997973799, -0.058064879979737964], 'target': 0.1602324071186423}\n",
      "48 {'train': [0.36660722392149514, 0.33072115454133333, 0.2624617942848886, 0.1685108449249893, 0.05806487997973799, -0.058064879979737964, -0.16851084492498963], 'target': 0.10287553806257692}\n",
      "49 {'train': [0.33072115454133333, 0.2624617942848886, 0.1685108449249893, 0.05806487997973799, -0.058064879979737964, -0.16851084492498963, -0.26246179428488825], 'target': 0.035448494564925555}\n",
      "50 {'train': [0.2624617942848886, 0.1685108449249893, 0.05806487997973799, -0.058064879979737964, -0.16851084492498963, -0.26246179428488825, -0.3307211545413338], 'target': -0.03544849456492554}\n",
      "51 {'train': [0.1685108449249893, 0.05806487997973799, -0.058064879979737964, -0.16851084492498963, -0.26246179428488825, -0.3307211545413338, -0.3666072239214943], 'target': -0.10287553806257711}\n",
      "52 {'train': [0.05806487997973799, -0.058064879979737964, -0.16851084492498963, -0.26246179428488825, -0.3307211545413338, -0.3666072239214943, -0.36660722392149514], 'target': -0.16023240711864215}\n",
      "53 {'train': [-0.058064879979737964, -0.16851084492498963, -0.26246179428488825, -0.3307211545413338, -0.3666072239214943, -0.36660722392149514, -0.3307211545413338], 'target': -0.20190461176110872}\n",
      "54 {'train': [-0.16851084492498963, -0.26246179428488825, -0.3307211545413338, -0.3666072239214943, -0.36660722392149514, -0.3307211545413338, -0.26246179428488836], 'target': -0.2238129862522482}\n",
      "55 {'train': [-0.26246179428488825, -0.3307211545413338, -0.3666072239214943, -0.36660722392149514, -0.3307211545413338, -0.26246179428488836, -0.16851084492498952], 'target': -0.22381298625224869}\n",
      "56 {'train': [-0.3307211545413338, -0.3666072239214943, -0.36660722392149514, -0.3307211545413338, -0.26246179428488836, -0.16851084492498952, -0.058064879979737694], 'target': -0.20190461176110872}\n",
      "57 {'train': [-0.3666072239214943, -0.36660722392149514, -0.3307211545413338, -0.26246179428488836, -0.16851084492498952, -0.058064879979737694, 0.0580648799797379], 'target': -0.16023240711864223}\n",
      "58 {'train': [-0.36660722392149514, -0.3307211545413338, -0.26246179428488836, -0.16851084492498952, -0.058064879979737694, 0.0580648799797379, 0.16851084492498944], 'target': -0.1028755380625771}\n",
      "59 {'train': [-0.3307211545413338, -0.26246179428488836, -0.16851084492498952, -0.058064879979737694, 0.0580648799797379, 0.16851084492498944, 0.26246179428488875], 'target': -0.035448494564925374}\n",
      "60 {'train': [-0.16023240711864223, -0.1028755380625771, -0.035448494564925374, 0.03544849456492551, 0.10287553806257702, 0.16023240711864242, 0.20190461176110852], 'target': 0.021641235930532368}\n",
      "61 {'train': [-0.1028755380625771, -0.035448494564925374, 0.03544849456492551, 0.10287553806257702, 0.16023240711864242, 0.20190461176110852, 0.22381298625224844], 'target': 0.06280531283535962}\n",
      "62 {'train': [-0.035448494564925374, 0.03544849456492551, 0.10287553806257702, 0.16023240711864242, 0.20190461176110852, 0.22381298625224844, 0.22381298625224869], 'target': 0.09782156812951649}\n",
      "63 {'train': [0.03544849456492551, 0.10287553806257702, 0.16023240711864242, 0.20190461176110852, 0.22381298625224844, 0.22381298625224869, 0.20190461176110852], 'target': 0.12326236677221424}\n",
      "64 {'train': [0.10287553806257702, 0.16023240711864242, 0.20190461176110852, 0.22381298625224844, 0.22381298625224869, 0.20190461176110852, 0.1602324071186423], 'target': 0.1366373861358387}\n",
      "65 {'train': [0.16023240711864242, 0.20190461176110852, 0.22381298625224844, 0.22381298625224869, 0.20190461176110852, 0.1602324071186423, 0.10287553806257692], 'target': 0.13663738613583884}\n",
      "66 {'train': [0.20190461176110852, 0.22381298625224844, 0.22381298625224869, 0.20190461176110852, 0.1602324071186423, 0.10287553806257692, 0.035448494564925555], 'target': 0.12326236677221421}\n",
      "67 {'train': [0.22381298625224844, 0.22381298625224869, 0.20190461176110852, 0.1602324071186423, 0.10287553806257692, 0.035448494564925555, -0.03544849456492554], 'target': 0.09782156812951645}\n",
      "68 {'train': [0.22381298625224869, 0.20190461176110852, 0.1602324071186423, 0.10287553806257692, 0.035448494564925555, -0.03544849456492554, -0.10287553806257711], 'target': 0.06280531283535958}\n",
      "69 {'train': [0.20190461176110852, 0.1602324071186423, 0.10287553806257692, 0.035448494564925555, -0.03544849456492554, -0.10287553806257711, -0.16023240711864215], 'target': 0.021641235930532403}\n",
      "70 {'train': [0.1602324071186423, 0.10287553806257692, 0.035448494564925555, -0.03544849456492554, -0.10287553806257711, -0.16023240711864215, -0.20190461176110872], 'target': -0.021641235930532392}\n",
      "71 {'train': [0.10287553806257692, 0.035448494564925555, -0.03544849456492554, -0.10287553806257711, -0.16023240711864215, -0.20190461176110872, -0.2238129862522482], 'target': -0.0628053128353597}\n",
      "72 {'train': [0.035448494564925555, -0.03544849456492554, -0.10287553806257711, -0.16023240711864215, -0.20190461176110872, -0.2238129862522482, -0.22381298625224869], 'target': -0.09782156812951631}\n",
      "73 {'train': [-0.03544849456492554, -0.10287553806257711, -0.16023240711864215, -0.20190461176110872, -0.2238129862522482, -0.22381298625224869, -0.20190461176110872], 'target': -0.12326236677221439}\n",
      "74 {'train': [-0.10287553806257711, -0.16023240711864215, -0.20190461176110872, -0.2238129862522482, -0.22381298625224869, -0.20190461176110872, -0.16023240711864223], 'target': -0.13663738613583856}\n",
      "75 {'train': [-0.16023240711864215, -0.20190461176110872, -0.2238129862522482, -0.22381298625224869, -0.20190461176110872, -0.16023240711864223, -0.1028755380625771], 'target': -0.13663738613583884}\n",
      "76 {'train': [-0.20190461176110872, -0.2238129862522482, -0.22381298625224869, -0.20190461176110872, -0.16023240711864223, -0.1028755380625771, -0.035448494564925374], 'target': -0.12326236677221436}\n",
      "77 {'train': [-0.2238129862522482, -0.22381298625224869, -0.20190461176110872, -0.16023240711864223, -0.1028755380625771, -0.035448494564925374, 0.03544849456492551], 'target': -0.09782156812951635}\n",
      "78 {'train': [-0.22381298625224869, -0.20190461176110872, -0.16023240711864223, -0.1028755380625771, -0.035448494564925374, 0.03544849456492551, 0.10287553806257702], 'target': -0.06280531283535967}\n",
      "79 {'train': [-0.20190461176110872, -0.16023240711864223, -0.1028755380625771, -0.035448494564925374, 0.03544849456492551, 0.10287553806257702, 0.16023240711864242], 'target': -0.02164123593053229}\n",
      "80 {'train': [-0.09782156812951635, -0.06280531283535967, -0.02164123593053229, 0.021641235930532368, 0.06280531283535962, 0.09782156812951649, 0.12326236677221424], 'target': 0.013211931799901233}\n",
      "81 {'train': [-0.06280531283535967, -0.02164123593053229, 0.021641235930532368, 0.06280531283535962, 0.09782156812951649, 0.12326236677221424, 0.1366373861358387], 'target': 0.038342519462187676}\n",
      "82 {'train': [-0.02164123593053229, 0.021641235930532368, 0.06280531283535962, 0.09782156812951649, 0.12326236677221424, 0.1366373861358387, 0.13663738613583884], 'target': 0.05971987417147347}\n",
      "83 {'train': [0.021641235930532368, 0.06280531283535962, 0.09782156812951649, 0.12326236677221424, 0.1366373861358387, 0.13663738613583884, 0.12326236677221421], 'target': 0.0752514315040252}\n",
      "84 {'train': [0.06280531283535962, 0.09782156812951649, 0.12326236677221424, 0.1366373861358387, 0.13663738613583884, 0.12326236677221421, 0.09782156812951645], 'target': 0.08341685441340974}\n",
      "85 {'train': [0.09782156812951649, 0.12326236677221424, 0.1366373861358387, 0.13663738613583884, 0.12326236677221421, 0.09782156812951645, 0.06280531283535958], 'target': 0.08341685441340982}\n",
      "86 {'train': [0.12326236677221424, 0.1366373861358387, 0.13663738613583884, 0.12326236677221421, 0.09782156812951645, 0.06280531283535958, 0.021641235930532403], 'target': 0.0752514315040252}\n",
      "87 {'train': [0.1366373861358387, 0.13663738613583884, 0.12326236677221421, 0.09782156812951645, 0.06280531283535958, 0.021641235930532403, -0.021641235930532392], 'target': 0.05971987417147344}\n",
      "88 {'train': [0.13663738613583884, 0.12326236677221421, 0.09782156812951645, 0.06280531283535958, 0.021641235930532403, -0.021641235930532392, -0.0628053128353597], 'target': 0.038342519462187655}\n",
      "89 {'train': [0.12326236677221421, 0.09782156812951645, 0.06280531283535958, 0.021641235930532403, -0.021641235930532392, -0.0628053128353597, -0.09782156812951631], 'target': 0.013211931799901249}\n",
      "90 {'train': [0.09782156812951645, 0.06280531283535958, 0.021641235930532403, -0.021641235930532392, -0.0628053128353597, -0.09782156812951631, -0.12326236677221439], 'target': -0.013211931799901242}\n",
      "91 {'train': [0.06280531283535958, 0.021641235930532403, -0.021641235930532392, -0.0628053128353597, -0.09782156812951631, -0.12326236677221439, -0.13663738613583856], 'target': -0.038342519462187724}\n",
      "92 {'train': [0.021641235930532403, -0.021641235930532392, -0.0628053128353597, -0.09782156812951631, -0.12326236677221439, -0.13663738613583856, -0.13663738613583884], 'target': -0.059719874171473356}\n",
      "93 {'train': [-0.021641235930532392, -0.0628053128353597, -0.09782156812951631, -0.12326236677221439, -0.13663738613583856, -0.13663738613583884, -0.12326236677221436], 'target': -0.07525143150402529}\n",
      "94 {'train': [-0.0628053128353597, -0.09782156812951631, -0.12326236677221439, -0.13663738613583856, -0.13663738613583884, -0.12326236677221436, -0.09782156812951635], 'target': -0.08341685441340965}\n",
      "95 {'train': [-0.09782156812951631, -0.12326236677221439, -0.13663738613583856, -0.13663738613583884, -0.12326236677221436, -0.09782156812951635, -0.06280531283535967], 'target': -0.08341685441340982}\n",
      "96 {'train': [-0.12326236677221439, -0.13663738613583856, -0.13663738613583884, -0.12326236677221436, -0.09782156812951635, -0.06280531283535967, -0.02164123593053229], 'target': -0.07525143150402529}\n",
      "97 {'train': [-0.13663738613583856, -0.13663738613583884, -0.12326236677221436, -0.09782156812951635, -0.06280531283535967, -0.02164123593053229, 0.021641235930532368], 'target': -0.05971987417147337}\n",
      "98 {'train': [-0.13663738613583884, -0.12326236677221436, -0.09782156812951635, -0.06280531283535967, -0.02164123593053229, 0.021641235930532368, 0.06280531283535962], 'target': -0.0383425194621877}\n",
      "99 {'train': [-0.12326236677221436, -0.09782156812951635, -0.06280531283535967, -0.02164123593053229, 0.021641235930532368, 0.06280531283535962, 0.09782156812951649], 'target': -0.013211931799901183}\n",
      "100 {'train': [-0.05971987417147337, -0.0383425194621877, -0.013211931799901183, 0.013211931799901233, 0.038342519462187676, 0.05971987417147347, 0.0752514315040252], 'target': 0.008065858273786086}\n",
      "101 {'train': [-0.0383425194621877, -0.013211931799901183, 0.013211931799901233, 0.038342519462187676, 0.05971987417147347, 0.0752514315040252, 0.08341685441340974], 'target': 0.023408032415380968}\n",
      "102 {'train': [-0.013211931799901183, 0.013211931799901233, 0.038342519462187676, 0.05971987417147347, 0.0752514315040252, 0.08341685441340974, 0.08341685441340982], 'target': 0.03645886525080641}\n",
      "103 {'train': [0.013211931799901233, 0.038342519462187676, 0.05971987417147347, 0.0752514315040252, 0.08341685441340974, 0.08341685441340982, 0.0752514315040252], 'target': 0.045940850331631776}\n",
      "104 {'train': [0.038342519462187676, 0.05971987417147347, 0.0752514315040252, 0.08341685441340974, 0.08341685441340982, 0.0752514315040252, 0.05971987417147344], 'target': 0.05092582489327112}\n",
      "105 {'train': [0.05971987417147347, 0.0752514315040252, 0.08341685441340974, 0.08341685441340982, 0.0752514315040252, 0.05971987417147344, 0.038342519462187655], 'target': 0.05092582489327118}\n",
      "106 {'train': [0.0752514315040252, 0.08341685441340974, 0.08341685441340982, 0.0752514315040252, 0.05971987417147344, 0.038342519462187655, 0.013211931799901249], 'target': 0.045940850331631755}\n",
      "107 {'train': [0.08341685441340974, 0.08341685441340982, 0.0752514315040252, 0.05971987417147344, 0.038342519462187655, 0.013211931799901249, -0.013211931799901242], 'target': 0.0364588652508064}\n",
      "108 {'train': [0.08341685441340982, 0.0752514315040252, 0.05971987417147344, 0.038342519462187655, 0.013211931799901249, -0.013211931799901242, -0.038342519462187724], 'target': 0.023408032415380944}\n",
      "109 {'train': [0.0752514315040252, 0.05971987417147344, 0.038342519462187655, 0.013211931799901249, -0.013211931799901242, -0.038342519462187724, -0.059719874171473356], 'target': 0.008065858273786098}\n",
      "110 {'train': [0.05971987417147344, 0.038342519462187655, 0.013211931799901249, -0.013211931799901242, -0.038342519462187724, -0.059719874171473356, -0.07525143150402529], 'target': -0.008065858273786093}\n",
      "111 {'train': [0.038342519462187655, 0.013211931799901249, -0.013211931799901242, -0.038342519462187724, -0.059719874171473356, -0.07525143150402529, -0.08341685441340965], 'target': -0.02340803241538099}\n",
      "112 {'train': [0.013211931799901249, -0.013211931799901242, -0.038342519462187724, -0.059719874171473356, -0.07525143150402529, -0.08341685441340965, -0.08341685441340982], 'target': -0.03645886525080635}\n",
      "113 {'train': [-0.013211931799901242, -0.038342519462187724, -0.059719874171473356, -0.07525143150402529, -0.08341685441340965, -0.08341685441340982, -0.07525143150402529], 'target': -0.045940850331631825}\n",
      "114 {'train': [-0.038342519462187724, -0.059719874171473356, -0.07525143150402529, -0.08341685441340965, -0.08341685441340982, -0.07525143150402529, -0.05971987417147337], 'target': -0.05092582489327108}\n",
      "115 {'train': [-0.059719874171473356, -0.07525143150402529, -0.08341685441340965, -0.08341685441340982, -0.07525143150402529, -0.05971987417147337, -0.0383425194621877], 'target': -0.05092582489327118}\n",
      "116 {'train': [-0.07525143150402529, -0.08341685441340965, -0.08341685441340982, -0.07525143150402529, -0.05971987417147337, -0.0383425194621877, -0.013211931799901183], 'target': -0.04594085033163181}\n",
      "117 {'train': [-0.08341685441340965, -0.08341685441340982, -0.07525143150402529, -0.05971987417147337, -0.0383425194621877, -0.013211931799901183, 0.013211931799901233], 'target': -0.03645886525080636}\n",
      "118 {'train': [-0.08341685441340982, -0.07525143150402529, -0.05971987417147337, -0.0383425194621877, -0.013211931799901183, 0.013211931799901233, 0.038342519462187676], 'target': -0.023408032415380982}\n",
      "119 {'train': [-0.07525143150402529, -0.05971987417147337, -0.0383425194621877, -0.013211931799901183, 0.013211931799901233, 0.038342519462187676, 0.05971987417147347], 'target': -0.008065858273786055}\n",
      "120 {'train': [-0.03645886525080636, -0.023408032415380982, -0.008065858273786055, 0.008065858273786086, 0.023408032415380968, 0.03645886525080641, 0.045940850331631776], 'target': 0.004924190548220198}\n",
      "121 {'train': [-0.023408032415380982, -0.008065858273786055, 0.008065858273786086, 0.023408032415380968, 0.03645886525080641, 0.045940850331631776, 0.05092582489327112], 'target': 0.014290557564947846}\n",
      "122 {'train': [-0.008065858273786055, 0.008065858273786086, 0.023408032415380968, 0.03645886525080641, 0.045940850331631776, 0.05092582489327112, 0.05092582489327118], 'target': 0.022258065239049107}\n",
      "123 {'train': [0.008065858273786086, 0.023408032415380968, 0.03645886525080641, 0.045940850331631776, 0.05092582489327112, 0.05092582489327118, 0.045940850331631755], 'target': 0.028046798406492747}\n",
      "124 {'train': [0.023408032415380968, 0.03645886525080641, 0.045940850331631776, 0.05092582489327112, 0.05092582489327118, 0.045940850331631755, 0.0364588652508064], 'target': 0.031090115532373796}\n",
      "125 {'train': [0.03645886525080641, 0.045940850331631776, 0.05092582489327112, 0.05092582489327118, 0.045940850331631755, 0.0364588652508064, 0.023408032415380944], 'target': 0.03109011553237383}\n",
      "126 {'train': [0.045940850331631776, 0.05092582489327112, 0.05092582489327118, 0.045940850331631755, 0.0364588652508064, 0.023408032415380944, 0.008065858273786098], 'target': 0.02804679840649273}\n",
      "127 {'train': [0.05092582489327112, 0.05092582489327118, 0.045940850331631755, 0.0364588652508064, 0.023408032415380944, 0.008065858273786098, -0.008065858273786093], 'target': 0.0222580652390491}\n",
      "128 {'train': [0.05092582489327118, 0.045940850331631755, 0.0364588652508064, 0.023408032415380944, 0.008065858273786098, -0.008065858273786093, -0.02340803241538099], 'target': 0.014290557564947836}\n",
      "129 {'train': [0.045940850331631755, 0.0364588652508064, 0.023408032415380944, 0.008065858273786098, -0.008065858273786093, -0.02340803241538099, -0.03645886525080635], 'target': 0.004924190548220204}\n",
      "130 {'train': [0.0364588652508064, 0.023408032415380944, 0.008065858273786098, -0.008065858273786093, -0.02340803241538099, -0.03645886525080635, -0.045940850331631825], 'target': -0.004924190548220201}\n",
      "131 {'train': [0.023408032415380944, 0.008065858273786098, -0.008065858273786093, -0.02340803241538099, -0.03645886525080635, -0.045940850331631825, -0.05092582489327108], 'target': -0.014290557564947863}\n",
      "132 {'train': [0.008065858273786098, -0.008065858273786093, -0.02340803241538099, -0.03645886525080635, -0.045940850331631825, -0.05092582489327108, -0.05092582489327118], 'target': -0.022258065239049073}\n",
      "133 {'train': [-0.008065858273786093, -0.02340803241538099, -0.03645886525080635, -0.045940850331631825, -0.05092582489327108, -0.05092582489327118, -0.04594085033163181], 'target': -0.028046798406492775}\n",
      "134 {'train': [-0.02340803241538099, -0.03645886525080635, -0.045940850331631825, -0.05092582489327108, -0.05092582489327118, -0.04594085033163181, -0.03645886525080636], 'target': -0.031090115532373765}\n",
      "135 {'train': [-0.03645886525080635, -0.045940850331631825, -0.05092582489327108, -0.05092582489327118, -0.04594085033163181, -0.03645886525080636, -0.023408032415380982], 'target': -0.031090115532373834}\n",
      "136 {'train': [-0.045940850331631825, -0.05092582489327108, -0.05092582489327118, -0.04594085033163181, -0.03645886525080636, -0.023408032415380982, -0.008065858273786055], 'target': -0.028046798406492764}\n",
      "137 {'train': [-0.05092582489327108, -0.05092582489327118, -0.04594085033163181, -0.03645886525080636, -0.023408032415380982, -0.008065858273786055, 0.008065858273786086], 'target': -0.02225806523904908}\n",
      "138 {'train': [-0.05092582489327118, -0.04594085033163181, -0.03645886525080636, -0.023408032415380982, -0.008065858273786055, 0.008065858273786086, 0.023408032415380968], 'target': -0.014290557564947856}\n",
      "139 {'train': [-0.04594085033163181, -0.03645886525080636, -0.023408032415380982, -0.008065858273786055, 0.008065858273786086, 0.023408032415380968, 0.03645886525080641], 'target': -0.00492419054822018}\n",
      "140 {'train': [-0.02225806523904908, -0.014290557564947856, -0.00492419054822018, 0.004924190548220198, 0.014290557564947846, 0.022258065239049107, 0.028046798406492747], 'target': 0.0030062086057209354}\n",
      "141 {'train': [-0.014290557564947856, -0.00492419054822018, 0.004924190548220198, 0.014290557564947846, 0.022258065239049107, 0.028046798406492747, 0.031090115532373796], 'target': 0.008724357173347862}\n",
      "142 {'train': [-0.00492419054822018, 0.004924190548220198, 0.014290557564947846, 0.022258065239049107, 0.028046798406492747, 0.031090115532373796, 0.03109011553237383], 'target': 0.01358850487467677}\n",
      "143 {'train': [0.004924190548220198, 0.014290557564947846, 0.022258065239049107, 0.028046798406492747, 0.031090115532373796, 0.03109011553237383, 0.02804679840649273], 'target': 0.017122515042191727}\n",
      "144 {'train': [0.014290557564947846, 0.022258065239049107, 0.028046798406492747, 0.031090115532373796, 0.03109011553237383, 0.02804679840649273, 0.0222580652390491], 'target': 0.0189804541377997}\n",
      "145 {'train': [0.022258065239049107, 0.028046798406492747, 0.031090115532373796, 0.03109011553237383, 0.02804679840649273, 0.0222580652390491, 0.014290557564947836], 'target': 0.01898045413779972}\n",
      "146 {'train': [0.028046798406492747, 0.031090115532373796, 0.03109011553237383, 0.02804679840649273, 0.0222580652390491, 0.014290557564947836, 0.004924190548220204], 'target': 0.017122515042191727}\n",
      "147 {'train': [0.031090115532373796, 0.03109011553237383, 0.02804679840649273, 0.0222580652390491, 0.014290557564947836, 0.004924190548220204, -0.004924190548220201], 'target': 0.013588504874676763}\n",
      "148 {'train': [0.03109011553237383, 0.02804679840649273, 0.0222580652390491, 0.014290557564947836, 0.004924190548220204, -0.004924190548220201, -0.014290557564947863], 'target': 0.008724357173347857}\n",
      "149 {'train': [0.02804679840649273, 0.0222580652390491, 0.014290557564947836, 0.004924190548220204, -0.004924190548220201, -0.014290557564947863, -0.022258065239049073], 'target': 0.0030062086057209398}\n",
      "150 {'train': [0.0222580652390491, 0.014290557564947836, 0.004924190548220204, -0.004924190548220201, -0.014290557564947863, -0.022258065239049073, -0.028046798406492775], 'target': -0.0030062086057209376}\n",
      "151 {'train': [0.014290557564947836, 0.004924190548220204, -0.004924190548220201, -0.014290557564947863, -0.022258065239049073, -0.028046798406492775, -0.031090115532373765], 'target': -0.008724357173347873}\n",
      "152 {'train': [0.004924190548220204, -0.004924190548220201, -0.014290557564947863, -0.022258065239049073, -0.028046798406492775, -0.031090115532373765, -0.031090115532373834], 'target': -0.013588504874676747}\n",
      "153 {'train': [-0.004924190548220201, -0.014290557564947863, -0.022258065239049073, -0.028046798406492775, -0.031090115532373765, -0.031090115532373834, -0.028046798406492764], 'target': -0.017122515042191744}\n",
      "154 {'train': [-0.014290557564947863, -0.022258065239049073, -0.028046798406492775, -0.031090115532373765, -0.031090115532373834, -0.028046798406492764, -0.02225806523904908], 'target': -0.018980454137799682}\n",
      "155 {'train': [-0.022258065239049073, -0.028046798406492775, -0.031090115532373765, -0.031090115532373834, -0.028046798406492764, -0.02225806523904908, -0.014290557564947856], 'target': -0.01898045413779972}\n",
      "156 {'train': [-0.028046798406492775, -0.031090115532373765, -0.031090115532373834, -0.028046798406492764, -0.02225806523904908, -0.014290557564947856, -0.00492419054822018], 'target': -0.017122515042191744}\n",
      "157 {'train': [-0.031090115532373765, -0.031090115532373834, -0.028046798406492764, -0.02225806523904908, -0.014290557564947856, -0.00492419054822018, 0.004924190548220198], 'target': -0.013588504874676754}\n",
      "158 {'train': [-0.031090115532373834, -0.028046798406492764, -0.02225806523904908, -0.014290557564947856, -0.00492419054822018, 0.004924190548220198, 0.014290557564947846], 'target': -0.00872435717334787}\n",
      "159 {'train': [-0.028046798406492764, -0.02225806523904908, -0.014290557564947856, -0.00492419054822018, 0.004924190548220198, 0.014290557564947846, 0.022258065239049107], 'target': -0.0030062086057209237}\n",
      "160 {'train': [-0.013588504874676754, -0.00872435717334787, -0.0030062086057209237, 0.0030062086057209354, 0.008724357173347862, 0.01358850487467677, 0.017122515042191727], 'target': 0.0018352844173296766}\n",
      "161 {'train': [-0.00872435717334787, -0.0030062086057209237, 0.0030062086057209354, 0.008724357173347862, 0.01358850487467677, 0.017122515042191727, 0.0189804541377997], 'target': 0.005326202826042362}\n",
      "162 {'train': [-0.0030062086057209237, 0.0030062086057209354, 0.008724357173347862, 0.01358850487467677, 0.017122515042191727, 0.0189804541377997, 0.01898045413779972], 'target': 0.00829575539230483}\n",
      "163 {'train': [0.0030062086057209354, 0.008724357173347862, 0.01358850487467677, 0.017122515042191727, 0.0189804541377997, 0.01898045413779972, 0.017122515042191727], 'target': 0.010453261620841961}\n",
      "164 {'train': [0.008724357173347862, 0.01358850487467677, 0.017122515042191727, 0.0189804541377997, 0.01898045413779972, 0.017122515042191727, 0.013588504874676763], 'target': 0.011587529769774745}\n",
      "165 {'train': [0.01358850487467677, 0.017122515042191727, 0.0189804541377997, 0.01898045413779972, 0.017122515042191727, 0.013588504874676763, 0.008724357173347857], 'target': 0.011587529769774757}\n",
      "166 {'train': [0.017122515042191727, 0.0189804541377997, 0.01898045413779972, 0.017122515042191727, 0.013588504874676763, 0.008724357173347857, 0.0030062086057209398], 'target': 0.010453261620841958}\n",
      "167 {'train': [0.0189804541377997, 0.01898045413779972, 0.017122515042191727, 0.013588504874676763, 0.008724357173347857, 0.0030062086057209398, -0.0030062086057209376], 'target': 0.008295755392304824}\n",
      "168 {'train': [0.01898045413779972, 0.017122515042191727, 0.013588504874676763, 0.008724357173347857, 0.0030062086057209398, -0.0030062086057209376, -0.008724357173347873], 'target': 0.005326202826042358}\n",
      "169 {'train': [0.017122515042191727, 0.013588504874676763, 0.008724357173347857, 0.0030062086057209398, -0.0030062086057209376, -0.008724357173347873, -0.013588504874676747], 'target': 0.0018352844173296788}\n",
      "170 {'train': [0.013588504874676763, 0.008724357173347857, 0.0030062086057209398, -0.0030062086057209376, -0.008724357173347873, -0.013588504874676747, -0.017122515042191744], 'target': -0.001835284417329678}\n",
      "171 {'train': [0.008724357173347857, 0.0030062086057209398, -0.0030062086057209376, -0.008724357173347873, -0.013588504874676747, -0.017122515042191744, -0.018980454137799682], 'target': -0.0053262028260423686}\n",
      "172 {'train': [0.0030062086057209398, -0.0030062086057209376, -0.008724357173347873, -0.013588504874676747, -0.017122515042191744, -0.018980454137799682, -0.01898045413779972], 'target': -0.008295755392304812}\n",
      "173 {'train': [-0.0030062086057209376, -0.008724357173347873, -0.013588504874676747, -0.017122515042191744, -0.018980454137799682, -0.01898045413779972, -0.017122515042191744], 'target': -0.010453261620841973}\n",
      "174 {'train': [-0.008724357173347873, -0.013588504874676747, -0.017122515042191744, -0.018980454137799682, -0.01898045413779972, -0.017122515042191744, -0.013588504874676754], 'target': -0.011587529769774733}\n",
      "175 {'train': [-0.013588504874676747, -0.017122515042191744, -0.018980454137799682, -0.01898045413779972, -0.017122515042191744, -0.013588504874676754, -0.00872435717334787], 'target': -0.011587529769774757}\n",
      "176 {'train': [-0.017122515042191744, -0.018980454137799682, -0.01898045413779972, -0.017122515042191744, -0.013588504874676754, -0.00872435717334787, -0.0030062086057209237], 'target': -0.01045326162084197}\n",
      "177 {'train': [-0.018980454137799682, -0.01898045413779972, -0.017122515042191744, -0.013588504874676754, -0.00872435717334787, -0.0030062086057209237, 0.0030062086057209354], 'target': -0.008295755392304817}\n",
      "178 {'train': [-0.01898045413779972, -0.017122515042191744, -0.013588504874676754, -0.00872435717334787, -0.0030062086057209237, 0.0030062086057209354, 0.008724357173347862], 'target': -0.005326202826042366}\n",
      "179 {'train': [-0.017122515042191744, -0.013588504874676754, -0.00872435717334787, -0.0030062086057209237, 0.0030062086057209354, 0.008724357173347862, 0.01358850487467677], 'target': -0.00183528441732967}\n",
      "180 {'train': [-0.008295755392304817, -0.005326202826042366, -0.00183528441732967, 0.0018352844173296766, 0.005326202826042362, 0.00829575539230483, 0.010453261620841961], 'target': 0.0011204375125808568}\n",
      "181 {'train': [-0.005326202826042366, -0.00183528441732967, 0.0018352844173296766, 0.005326202826042362, 0.00829575539230483, 0.010453261620841961, 0.011587529769774745], 'target': 0.0032516363074639705}\n",
      "182 {'train': [-0.00183528441732967, 0.0018352844173296766, 0.005326202826042362, 0.00829575539230483, 0.010453261620841961, 0.011587529769774745, 0.011587529769774757], 'target': 0.005064542285090186}\n",
      "183 {'train': [0.0018352844173296766, 0.005326202826042362, 0.00829575539230483, 0.010453261620841961, 0.011587529769774745, 0.011587529769774757, 0.010453261620841958], 'target': 0.0063816955771107626}\n",
      "184 {'train': [0.005326202826042362, 0.00829575539230483, 0.010453261620841961, 0.011587529769774745, 0.011587529769774757, 0.010453261620841958, 0.008295755392304824], 'target': 0.007074164042156121}\n",
      "185 {'train': [0.00829575539230483, 0.010453261620841961, 0.011587529769774745, 0.011587529769774757, 0.010453261620841958, 0.008295755392304824, 0.005326202826042358], 'target': 0.007074164042156128}\n",
      "186 {'train': [0.010453261620841961, 0.011587529769774745, 0.011587529769774757, 0.010453261620841958, 0.008295755392304824, 0.005326202826042358, 0.0018352844173296788], 'target': 0.006381695577110761}\n",
      "187 {'train': [0.011587529769774745, 0.011587529769774757, 0.010453261620841958, 0.008295755392304824, 0.005326202826042358, 0.0018352844173296788, -0.001835284417329678], 'target': 0.005064542285090182}\n",
      "188 {'train': [0.011587529769774757, 0.010453261620841958, 0.008295755392304824, 0.005326202826042358, 0.0018352844173296788, -0.001835284417329678, -0.0053262028260423686], 'target': 0.003251636307463968}\n",
      "189 {'train': [0.010453261620841958, 0.008295755392304824, 0.005326202826042358, 0.0018352844173296788, -0.001835284417329678, -0.0053262028260423686, -0.008295755392304812], 'target': 0.0011204375125808584}\n",
      "190 {'train': [0.008295755392304824, 0.005326202826042358, 0.0018352844173296788, -0.001835284417329678, -0.0053262028260423686, -0.008295755392304812, -0.010453261620841973], 'target': -0.0011204375125808577}\n",
      "191 {'train': [0.005326202826042358, 0.0018352844173296788, -0.001835284417329678, -0.0053262028260423686, -0.008295755392304812, -0.010453261620841973, -0.011587529769774733], 'target': -0.0032516363074639744}\n",
      "192 {'train': [0.0018352844173296788, -0.001835284417329678, -0.0053262028260423686, -0.008295755392304812, -0.010453261620841973, -0.011587529769774733, -0.011587529769774757], 'target': -0.005064542285090176}\n",
      "193 {'train': [-0.001835284417329678, -0.0053262028260423686, -0.008295755392304812, -0.010453261620841973, -0.011587529769774733, -0.011587529769774757, -0.01045326162084197], 'target': -0.0063816955771107695}\n",
      "194 {'train': [-0.0053262028260423686, -0.008295755392304812, -0.010453261620841973, -0.011587529769774733, -0.011587529769774757, -0.01045326162084197, -0.008295755392304817], 'target': -0.007074164042156114}\n",
      "195 {'train': [-0.008295755392304812, -0.010453261620841973, -0.011587529769774733, -0.011587529769774757, -0.01045326162084197, -0.008295755392304817, -0.005326202826042366], 'target': -0.007074164042156129}\n",
      "196 {'train': [-0.010453261620841973, -0.011587529769774733, -0.011587529769774757, -0.01045326162084197, -0.008295755392304817, -0.005326202826042366, -0.00183528441732967], 'target': -0.0063816955771107695}\n",
      "197 {'train': [-0.011587529769774733, -0.011587529769774757, -0.01045326162084197, -0.008295755392304817, -0.005326202826042366, -0.00183528441732967, 0.0018352844173296766], 'target': -0.005064542285090179}\n",
      "198 {'train': [-0.011587529769774757, -0.01045326162084197, -0.008295755392304817, -0.005326202826042366, -0.00183528441732967, 0.0018352844173296766, 0.005326202826042362], 'target': -0.003251636307463973}\n",
      "199 {'train': [-0.01045326162084197, -0.008295755392304817, -0.005326202826042366, -0.00183528441732967, 0.0018352844173296766, 0.005326202826042362, 0.00829575539230483], 'target': -0.0011204375125808525}\n",
      "200 {'train': [-0.005064542285090179, -0.003251636307463973, -0.0011204375125808525, 0.0011204375125808568, 0.0032516363074639705, 0.005064542285090186, 0.0063816955771107626], 'target': 0.0006840248888643348}\n",
      "201 {'train': [-0.003251636307463973, -0.0011204375125808525, 0.0011204375125808568, 0.0032516363074639705, 0.005064542285090186, 0.0063816955771107626, 0.007074164042156121], 'target': 0.001985117544589322}\n",
      "202 {'train': [-0.0011204375125808525, 0.0011204375125808568, 0.0032516363074639705, 0.005064542285090186, 0.0063816955771107626, 0.007074164042156121, 0.007074164042156128], 'target': 0.003091893063922686}\n",
      "203 {'train': [0.0011204375125808568, 0.0032516363074639705, 0.005064542285090186, 0.0063816955771107626, 0.007074164042156121, 0.007074164042156128, 0.006381695577110761], 'target': 0.003896012547673593}\n",
      "204 {'train': [0.0032516363074639705, 0.005064542285090186, 0.0063816955771107626, 0.007074164042156121, 0.007074164042156128, 0.006381695577110761, 0.005064542285090182], 'target': 0.004318763178142621}\n",
      "205 {'train': [0.005064542285090186, 0.0063816955771107626, 0.007074164042156121, 0.007074164042156128, 0.006381695577110761, 0.005064542285090182, 0.003251636307463968], 'target': 0.004318763178142625}\n",
      "206 {'train': [0.0063816955771107626, 0.007074164042156121, 0.007074164042156128, 0.006381695577110761, 0.005064542285090182, 0.003251636307463968, 0.0011204375125808584], 'target': 0.0038960125476735913}\n",
      "207 {'train': [0.007074164042156121, 0.007074164042156128, 0.006381695577110761, 0.005064542285090182, 0.003251636307463968, 0.0011204375125808584, -0.0011204375125808577], 'target': 0.003091893063922684}\n",
      "208 {'train': [0.007074164042156128, 0.006381695577110761, 0.005064542285090182, 0.003251636307463968, 0.0011204375125808584, -0.0011204375125808577, -0.0032516363074639744], 'target': 0.001985117544589321}\n",
      "209 {'train': [0.006381695577110761, 0.005064542285090182, 0.003251636307463968, 0.0011204375125808584, -0.0011204375125808577, -0.0032516363074639744, -0.005064542285090176], 'target': 0.0006840248888643357}\n",
      "210 {'train': [0.005064542285090182, 0.003251636307463968, 0.0011204375125808584, -0.0011204375125808577, -0.0032516363074639744, -0.005064542285090176, -0.0063816955771107695], 'target': -0.0006840248888643352}\n",
      "211 {'train': [0.003251636307463968, 0.0011204375125808584, -0.0011204375125808577, -0.0032516363074639744, -0.005064542285090176, -0.0063816955771107695, -0.007074164042156114], 'target': -0.001985117544589325}\n",
      "212 {'train': [0.0011204375125808584, -0.0011204375125808577, -0.0032516363074639744, -0.005064542285090176, -0.0063816955771107695, -0.007074164042156114, -0.007074164042156129], 'target': -0.0030918930639226806}\n",
      "213 {'train': [-0.0011204375125808577, -0.0032516363074639744, -0.005064542285090176, -0.0063816955771107695, -0.007074164042156114, -0.007074164042156129, -0.0063816955771107695], 'target': -0.0038960125476735965}\n",
      "214 {'train': [-0.0032516363074639744, -0.005064542285090176, -0.0063816955771107695, -0.007074164042156114, -0.007074164042156129, -0.0063816955771107695, -0.005064542285090179], 'target': -0.004318763178142616}\n",
      "215 {'train': [-0.005064542285090176, -0.0063816955771107695, -0.007074164042156114, -0.007074164042156129, -0.0063816955771107695, -0.005064542285090179, -0.003251636307463973], 'target': -0.004318763178142625}\n",
      "216 {'train': [-0.0063816955771107695, -0.007074164042156114, -0.007074164042156129, -0.0063816955771107695, -0.005064542285090179, -0.003251636307463973, -0.0011204375125808525], 'target': -0.0038960125476735965}\n",
      "217 {'train': [-0.007074164042156114, -0.007074164042156129, -0.0063816955771107695, -0.005064542285090179, -0.003251636307463973, -0.0011204375125808525, 0.0011204375125808568], 'target': -0.003091893063922682}\n",
      "218 {'train': [-0.007074164042156129, -0.0063816955771107695, -0.005064542285090179, -0.003251636307463973, -0.0011204375125808525, 0.0011204375125808568, 0.0032516363074639705], 'target': -0.001985117544589324}\n",
      "219 {'train': [-0.0063816955771107695, -0.005064542285090179, -0.003251636307463973, -0.0011204375125808525, 0.0011204375125808568, 0.0032516363074639705, 0.005064542285090186], 'target': -0.0006840248888643324}\n",
      "220 {'train': [-0.003091893063922682, -0.001985117544589324, -0.0006840248888643324, 0.0006840248888643348, 0.001985117544589322, 0.003091893063922686, 0.003896012547673593], 'target': 0.0004175958438843323}\n",
      "221 {'train': [-0.001985117544589324, -0.0006840248888643324, 0.0006840248888643348, 0.001985117544589322, 0.003091893063922686, 0.003896012547673593, 0.004318763178142621], 'target': 0.0012119103408922685}\n",
      "222 {'train': [-0.0006840248888643324, 0.0006840248888643348, 0.001985117544589322, 0.003091893063922686, 0.003896012547673593, 0.004318763178142621, 0.004318763178142625], 'target': 0.0018875946098578138}\n",
      "223 {'train': [0.0006840248888643348, 0.001985117544589322, 0.003091893063922686, 0.003896012547673593, 0.004318763178142621, 0.004318763178142625, 0.0038960125476735913], 'target': 0.0023785079667654934}\n",
      "224 {'train': [0.001985117544589322, 0.003091893063922686, 0.003896012547673593, 0.004318763178142621, 0.004318763178142625, 0.0038960125476735913, 0.003091893063922684], 'target': 0.002636596391846706}\n",
      "225 {'train': [0.003091893063922686, 0.003896012547673593, 0.004318763178142621, 0.004318763178142625, 0.0038960125476735913, 0.003091893063922684, 0.001985117544589321], 'target': 0.0026365963918467093}\n",
      "226 {'train': [0.003896012547673593, 0.004318763178142621, 0.004318763178142625, 0.0038960125476735913, 0.003091893063922684, 0.001985117544589321, 0.0006840248888643357], 'target': 0.002378507966765493}\n",
      "227 {'train': [0.004318763178142621, 0.004318763178142625, 0.0038960125476735913, 0.003091893063922684, 0.001985117544589321, 0.0006840248888643357, -0.0006840248888643352], 'target': 0.0018875946098578127}\n",
      "228 {'train': [0.004318763178142625, 0.0038960125476735913, 0.003091893063922684, 0.001985117544589321, 0.0006840248888643357, -0.0006840248888643352, -0.001985117544589325], 'target': 0.0012119103408922678}\n",
      "229 {'train': [0.0038960125476735913, 0.003091893063922684, 0.001985117544589321, 0.0006840248888643357, -0.0006840248888643352, -0.001985117544589325, -0.0030918930639226806], 'target': 0.00041759584388433295}\n",
      "230 {'train': [0.003091893063922684, 0.001985117544589321, 0.0006840248888643357, -0.0006840248888643352, -0.001985117544589325, -0.0030918930639226806, -0.0038960125476735965], 'target': -0.0004175958438843328}\n",
      "231 {'train': [0.001985117544589321, 0.0006840248888643357, -0.0006840248888643352, -0.001985117544589325, -0.0030918930639226806, -0.0038960125476735965, -0.004318763178142616], 'target': -0.00121191034089227}\n",
      "232 {'train': [0.0006840248888643357, -0.0006840248888643352, -0.001985117544589325, -0.0030918930639226806, -0.0038960125476735965, -0.004318763178142616, -0.004318763178142625], 'target': -0.0018875946098578106}\n",
      "233 {'train': [-0.0006840248888643352, -0.001985117544589325, -0.0030918930639226806, -0.0038960125476735965, -0.004318763178142616, -0.004318763178142625, -0.0038960125476735965], 'target': -0.0023785079667654955}\n",
      "234 {'train': [-0.001985117544589325, -0.0030918930639226806, -0.0038960125476735965, -0.004318763178142616, -0.004318763178142625, -0.0038960125476735965, -0.003091893063922682], 'target': -0.0026365963918467033}\n",
      "235 {'train': [-0.0030918930639226806, -0.0038960125476735965, -0.004318763178142616, -0.004318763178142625, -0.0038960125476735965, -0.003091893063922682, -0.001985117544589324], 'target': -0.0026365963918467093}\n",
      "236 {'train': [-0.0038960125476735965, -0.004318763178142616, -0.004318763178142625, -0.0038960125476735965, -0.003091893063922682, -0.001985117544589324, -0.0006840248888643324], 'target': -0.0023785079667654955}\n",
      "237 {'train': [-0.004318763178142616, -0.004318763178142625, -0.0038960125476735965, -0.003091893063922682, -0.001985117544589324, -0.0006840248888643324, 0.0006840248888643348], 'target': -0.0018875946098578106}\n",
      "238 {'train': [-0.004318763178142625, -0.0038960125476735965, -0.003091893063922682, -0.001985117544589324, -0.0006840248888643324, 0.0006840248888643348, 0.001985117544589322], 'target': -0.0012119103408922693}\n",
      "239 {'train': [-0.0038960125476735965, -0.003091893063922682, -0.001985117544589324, -0.0006840248888643324, 0.0006840248888643348, 0.001985117544589322, 0.003091893063922686], 'target': -0.00041759584388433084}\n",
      "240 {'train': [-0.0018875946098578106, -0.0012119103408922693, -0.00041759584388433084, 0.0004175958438843323, 0.0012119103408922685, 0.0018875946098578138, 0.0023785079667654934], 'target': 0.0002549414380505893}\n",
      "241 {'train': [-0.0012119103408922693, -0.00041759584388433084, 0.0004175958438843323, 0.0012119103408922685, 0.0018875946098578138, 0.0023785079667654934, 0.002636596391846706], 'target': 0.0007398688699139296}\n",
      "242 {'train': [-0.00041759584388433084, 0.0004175958438843323, 0.0012119103408922685, 0.0018875946098578138, 0.0023785079667654934, 0.002636596391846706, 0.0026365963918467093], 'target': 0.0011523727818205593}\n",
      "243 {'train': [0.0004175958438843323, 0.0012119103408922685, 0.0018875946098578138, 0.0023785079667654934, 0.002636596391846706, 0.0026365963918467093, 0.002378507966765493], 'target': 0.0014520744167893015}\n",
      "244 {'train': [0.0012119103408922685, 0.0018875946098578138, 0.0023785079667654934, 0.002636596391846706, 0.0026365963918467093, 0.002378507966765493, 0.0018875946098578127], 'target': 0.0016096368906453411}\n",
      "245 {'train': [0.0018875946098578138, 0.0023785079667654934, 0.002636596391846706, 0.0026365963918467093, 0.002378507966765493, 0.0018875946098578127, 0.0012119103408922678], 'target': 0.0016096368906453429}\n",
      "246 {'train': [0.0023785079667654934, 0.002636596391846706, 0.0026365963918467093, 0.002378507966765493, 0.0018875946098578127, 0.0012119103408922678, 0.00041759584388433295], 'target': 0.0014520744167893015}\n",
      "247 {'train': [0.002636596391846706, 0.0026365963918467093, 0.002378507966765493, 0.0018875946098578127, 0.0012119103408922678, 0.00041759584388433295, -0.0004175958438843328], 'target': 0.0011523727818205586}\n",
      "248 {'train': [0.0026365963918467093, 0.002378507966765493, 0.0018875946098578127, 0.0012119103408922678, 0.00041759584388433295, -0.0004175958438843328, -0.00121191034089227], 'target': 0.000739868869913929}\n",
      "249 {'train': [0.002378507966765493, 0.0018875946098578127, 0.0012119103408922678, 0.00041759584388433295, -0.0004175958438843328, -0.00121191034089227, -0.0018875946098578106], 'target': 0.00025494143805058963}\n",
      "250 {'train': [0.0018875946098578127, 0.0012119103408922678, 0.00041759584388433295, -0.0004175958438843328, -0.00121191034089227, -0.0018875946098578106, -0.0023785079667654955], 'target': -0.00025494143805058947}\n",
      "251 {'train': [0.0012119103408922678, 0.00041759584388433295, -0.0004175958438843328, -0.00121191034089227, -0.0018875946098578106, -0.0023785079667654955, -0.0026365963918467033], 'target': -0.0007398688699139306}\n",
      "252 {'train': [0.00041759584388433295, -0.0004175958438843328, -0.00121191034089227, -0.0018875946098578106, -0.0023785079667654955, -0.0026365963918467033, -0.0026365963918467093], 'target': -0.001152372781820557}\n",
      "253 {'train': [-0.0004175958438843328, -0.00121191034089227, -0.0018875946098578106, -0.0023785079667654955, -0.0026365963918467033, -0.0026365963918467093, -0.0023785079667654955], 'target': -0.001452074416789303}\n",
      "254 {'train': [-0.00121191034089227, -0.0018875946098578106, -0.0023785079667654955, -0.0026365963918467033, -0.0026365963918467093, -0.0023785079667654955, -0.0018875946098578106], 'target': -0.0016096368906453398}\n",
      "255 {'train': [-0.0018875946098578106, -0.0023785079667654955, -0.0026365963918467033, -0.0026365963918467093, -0.0023785079667654955, -0.0018875946098578106, -0.0012119103408922693], 'target': -0.0016096368906453429}\n",
      "256 {'train': [-0.0023785079667654955, -0.0026365963918467033, -0.0026365963918467093, -0.0023785079667654955, -0.0018875946098578106, -0.0012119103408922693, -0.00041759584388433084], 'target': -0.001452074416789303}\n",
      "257 {'train': [-0.0026365963918467033, -0.0026365963918467093, -0.0023785079667654955, -0.0018875946098578106, -0.0012119103408922693, -0.00041759584388433084, 0.0004175958438843323], 'target': -0.0011523727818205573}\n",
      "258 {'train': [-0.0026365963918467093, -0.0023785079667654955, -0.0018875946098578106, -0.0012119103408922693, -0.00041759584388433084, 0.0004175958438843323, 0.0012119103408922685], 'target': -0.0007398688699139302}\n",
      "259 {'train': [-0.0023785079667654955, -0.0018875946098578106, -0.0012119103408922693, -0.00041759584388433084, 0.0004175958438843323, 0.0012119103408922685, 0.0018875946098578138], 'target': -0.0002549414380505883}\n",
      "260 {'train': [-0.0011523727818205573, -0.0007398688699139302, -0.0002549414380505883, 0.0002549414380505893, 0.0007398688699139296, 0.0011523727818205593, 0.0014520744167893015], 'target': 0.00015564124448830716}\n",
      "261 {'train': [-0.0007398688699139302, -0.0002549414380505883, 0.0002549414380505893, 0.0007398688699139296, 0.0011523727818205593, 0.0014520744167893015, 0.0016096368906453411], 'target': 0.00045168848403809064}\n",
      "262 {'train': [-0.0002549414380505883, 0.0002549414380505893, 0.0007398688699139296, 0.0011523727818205593, 0.0014520744167893015, 0.0016096368906453411, 0.0016096368906453429], 'target': 0.0007035213076715043}\n",
      "263 {'train': [0.0002549414380505893, 0.0007398688699139296, 0.0011523727818205593, 0.0014520744167893015, 0.0016096368906453411, 0.0016096368906453429, 0.0014520744167893015], 'target': 0.0008864885639888519}\n",
      "264 {'train': [0.0007398688699139296, 0.0011523727818205593, 0.0014520744167893015, 0.0016096368906453411, 0.0016096368906453429, 0.0014520744167893015, 0.0011523727818205586], 'target': 0.000982680143133958}\n",
      "265 {'train': [0.0011523727818205593, 0.0014520744167893015, 0.0016096368906453411, 0.0016096368906453429, 0.0014520744167893015, 0.0011523727818205586, 0.000739868869913929], 'target': 0.000982680143133959}\n",
      "266 {'train': [0.0014520744167893015, 0.0016096368906453411, 0.0016096368906453429, 0.0014520744167893015, 0.0011523727818205586, 0.000739868869913929, 0.00025494143805058963], 'target': 0.0008864885639888519}\n",
      "267 {'train': [0.0016096368906453411, 0.0016096368906453429, 0.0014520744167893015, 0.0011523727818205586, 0.000739868869913929, 0.00025494143805058963, -0.00025494143805058947], 'target': 0.0007035213076715039}\n",
      "268 {'train': [0.0016096368906453429, 0.0014520744167893015, 0.0011523727818205586, 0.000739868869913929, 0.00025494143805058963, -0.00025494143805058947, -0.0007398688699139306], 'target': 0.0004516884840380903}\n",
      "269 {'train': [0.0014520744167893015, 0.0011523727818205586, 0.000739868869913929, 0.00025494143805058963, -0.00025494143805058947, -0.0007398688699139306, -0.001152372781820557], 'target': 0.00015564124448830735}\n",
      "270 {'train': [0.0011523727818205586, 0.000739868869913929, 0.00025494143805058963, -0.00025494143805058947, -0.0007398688699139306, -0.001152372781820557, -0.001452074416789303], 'target': -0.00015564124448830732}\n",
      "271 {'train': [0.000739868869913929, 0.00025494143805058963, -0.00025494143805058947, -0.0007398688699139306, -0.001152372781820557, -0.001452074416789303, -0.0016096368906453398], 'target': -0.0004516884840380912}\n",
      "272 {'train': [0.00025494143805058963, -0.00025494143805058947, -0.0007398688699139306, -0.001152372781820557, -0.001452074416789303, -0.0016096368906453398, -0.0016096368906453429], 'target': -0.0007035213076715028}\n",
      "273 {'train': [-0.00025494143805058947, -0.0007398688699139306, -0.001152372781820557, -0.001452074416789303, -0.0016096368906453398, -0.0016096368906453429, -0.001452074416789303], 'target': -0.0008864885639888529}\n",
      "274 {'train': [-0.0007398688699139306, -0.001152372781820557, -0.001452074416789303, -0.0016096368906453398, -0.0016096368906453429, -0.001452074416789303, -0.0011523727818205573], 'target': -0.0009826801431339568}\n",
      "275 {'train': [-0.001152372781820557, -0.001452074416789303, -0.0016096368906453398, -0.0016096368906453429, -0.001452074416789303, -0.0011523727818205573, -0.0007398688699139302], 'target': -0.000982680143133959}\n",
      "276 {'train': [-0.001452074416789303, -0.0016096368906453398, -0.0016096368906453429, -0.001452074416789303, -0.0011523727818205573, -0.0007398688699139302, -0.0002549414380505883], 'target': -0.0008864885639888529}\n",
      "277 {'train': [-0.0016096368906453398, -0.0016096368906453429, -0.001452074416789303, -0.0011523727818205573, -0.0007398688699139302, -0.0002549414380505883, 0.0002549414380505893], 'target': -0.0007035213076715032}\n",
      "278 {'train': [-0.0016096368906453429, -0.001452074416789303, -0.0011523727818205573, -0.0007398688699139302, -0.0002549414380505883, 0.0002549414380505893, 0.0007398688699139296], 'target': -0.0004516884840380909}\n",
      "279 {'train': [-0.001452074416789303, -0.0011523727818205573, -0.0007398688699139302, -0.0002549414380505883, 0.0002549414380505893, 0.0007398688699139296, 0.0011523727818205593], 'target': -0.00015564124448830662}\n",
      "280 {'train': [-0.0007035213076715032, -0.0004516884840380909, -0.00015564124448830662, 0.00015564124448830716, 0.00045168848403809064, 0.0007035213076715043, 0.0008864885639888519], 'target': 9.50186724100228e-05}\n",
      "281 {'train': [-0.0004516884840380909, -0.00015564124448830662, 0.00015564124448830716, 0.00045168848403809064, 0.0007035213076715043, 0.0008864885639888519, 0.000982680143133958], 'target': 0.0002757549275405561}\n",
      "282 {'train': [-0.00015564124448830662, 0.00015564124448830716, 0.00045168848403809064, 0.0007035213076715043, 0.0008864885639888519, 0.000982680143133958, 0.000982680143133959], 'target': 0.00042949836906586495}\n",
      "283 {'train': [0.00015564124448830716, 0.00045168848403809064, 0.0007035213076715043, 0.0008864885639888519, 0.000982680143133958, 0.000982680143133959, 0.0008864885639888519], 'target': 0.0005411995177359069}\n",
      "284 {'train': [0.00045168848403809064, 0.0007035213076715043, 0.0008864885639888519, 0.000982680143133958, 0.000982680143133959, 0.0008864885639888519, 0.0007035213076715039], 'target': 0.0005999242868511928}\n",
      "285 {'train': [0.0007035213076715043, 0.0008864885639888519, 0.000982680143133958, 0.000982680143133959, 0.0008864885639888519, 0.0007035213076715039, 0.0004516884840380903], 'target': 0.0005999242868511935}\n",
      "286 {'train': [0.0008864885639888519, 0.000982680143133958, 0.000982680143133959, 0.0008864885639888519, 0.0007035213076715039, 0.0004516884840380903, 0.00015564124448830735], 'target': 0.0005411995177359068}\n",
      "287 {'train': [0.000982680143133958, 0.000982680143133959, 0.0008864885639888519, 0.0007035213076715039, 0.0004516884840380903, 0.00015564124448830735, -0.00015564124448830732], 'target': 0.0004294983690658648}\n",
      "288 {'train': [0.000982680143133959, 0.0008864885639888519, 0.0007035213076715039, 0.0004516884840380903, 0.00015564124448830735, -0.00015564124448830732, -0.0004516884840380912], 'target': 0.00027575492754055593}\n",
      "289 {'train': [0.0008864885639888519, 0.0007035213076715039, 0.0004516884840380903, 0.00015564124448830735, -0.00015564124448830732, -0.0004516884840380912, -0.0007035213076715028], 'target': 9.501867241002288e-05}\n",
      "290 {'train': [0.0007035213076715039, 0.0004516884840380903, 0.00015564124448830735, -0.00015564124448830732, -0.0004516884840380912, -0.0007035213076715028, -0.0008864885639888529], 'target': -9.501867241002283e-05}\n",
      "291 {'train': [0.0004516884840380903, 0.00015564124448830735, -0.00015564124448830732, -0.0004516884840380912, -0.0007035213076715028, -0.0008864885639888529, -0.0009826801431339568], 'target': -0.0002757549275405564}\n",
      "292 {'train': [0.00015564124448830735, -0.00015564124448830732, -0.0004516884840380912, -0.0007035213076715028, -0.0008864885639888529, -0.0009826801431339568, -0.000982680143133959], 'target': -0.0004294983690658643}\n",
      "293 {'train': [-0.00015564124448830732, -0.0004516884840380912, -0.0007035213076715028, -0.0008864885639888529, -0.0009826801431339568, -0.000982680143133959, -0.0008864885639888529], 'target': -0.0005411995177359074}\n",
      "294 {'train': [-0.0004516884840380912, -0.0007035213076715028, -0.0008864885639888529, -0.0009826801431339568, -0.000982680143133959, -0.0008864885639888529, -0.0007035213076715032], 'target': -0.0005999242868511922}\n",
      "295 {'train': [-0.0007035213076715028, -0.0008864885639888529, -0.0009826801431339568, -0.000982680143133959, -0.0008864885639888529, -0.0007035213076715032, -0.0004516884840380909], 'target': -0.0005999242868511935}\n",
      "296 {'train': [-0.0008864885639888529, -0.0009826801431339568, -0.000982680143133959, -0.0008864885639888529, -0.0007035213076715032, -0.0004516884840380909, -0.00015564124448830662], 'target': -0.0005411995177359073}\n",
      "297 {'train': [-0.0009826801431339568, -0.000982680143133959, -0.0008864885639888529, -0.0007035213076715032, -0.0004516884840380909, -0.00015564124448830662, 0.00015564124448830716], 'target': -0.0004294983690658643}\n",
      "298 {'train': [-0.000982680143133959, -0.0008864885639888529, -0.0007035213076715032, -0.0004516884840380909, -0.00015564124448830662, 0.00015564124448830716, 0.00045168848403809064], 'target': -0.0002757549275405564}\n",
      "299 {'train': [-0.0008864885639888529, -0.0007035213076715032, -0.0004516884840380909, -0.00015564124448830662, 0.00015564124448830716, 0.00045168848403809064, 0.0007035213076715043], 'target': -9.50186724100224e-05}\n",
      "300 {'train': [-0.0004294983690658643, -0.0002757549275405564, -9.50186724100224e-05, 9.50186724100228e-05, 0.0002757549275405561, 0.00042949836906586495, 0.0005411995177359069], 'target': 5.80087118696966e-05}\n",
      "301 {'train': [-0.0002757549275405564, -9.50186724100224e-05, 9.50186724100228e-05, 0.0002757549275405561, 0.00042949836906586495, 0.0005411995177359069, 0.0005999242868511928], 'target': 0.00016834783872082254}\n",
      "302 {'train': [-9.50186724100224e-05, 9.50186724100228e-05, 0.0002757549275405561, 0.00042949836906586495, 0.0005411995177359069, 0.0005999242868511928, 0.0005999242868511935], 'target': 0.00026220790616959106}\n",
      "303 {'train': [9.50186724100228e-05, 0.0002757549275405561, 0.00042949836906586495, 0.0005411995177359069, 0.0005999242868511928, 0.0005999242868511935, 0.0005411995177359068], 'target': 0.00033040123685257295}\n",
      "304 {'train': [0.0002757549275405561, 0.00042949836906586495, 0.0005411995177359069, 0.0005999242868511928, 0.0005999242868511935, 0.0005411995177359068, 0.0004294983690658648], 'target': 0.0003662525924316449}\n",
      "305 {'train': [0.00042949836906586495, 0.0005411995177359069, 0.0005999242868511928, 0.0005999242868511935, 0.0005411995177359068, 0.0004294983690658648, 0.00027575492754055593], 'target': 0.00036625259243164526}\n",
      "306 {'train': [0.0005411995177359069, 0.0005999242868511928, 0.0005999242868511935, 0.0005411995177359068, 0.0004294983690658648, 0.00027575492754055593, 9.501867241002288e-05], 'target': 0.0003304012368525729}\n",
      "307 {'train': [0.0005999242868511928, 0.0005999242868511935, 0.0005411995177359068, 0.0004294983690658648, 0.00027575492754055593, 9.501867241002288e-05, -9.501867241002283e-05], 'target': 0.000262207906169591}\n",
      "308 {'train': [0.0005999242868511935, 0.0005411995177359068, 0.0004294983690658648, 0.00027575492754055593, 9.501867241002288e-05, -9.501867241002283e-05, -0.0002757549275405564], 'target': 0.00016834783872082243}\n",
      "309 {'train': [0.0005411995177359068, 0.0004294983690658648, 0.00027575492754055593, 9.501867241002288e-05, -9.501867241002283e-05, -0.0002757549275405564, -0.0004294983690658643], 'target': 5.800871186969667e-05}\n",
      "310 {'train': [0.0004294983690658648, 0.00027575492754055593, 9.501867241002288e-05, -9.501867241002283e-05, -0.0002757549275405564, -0.0004294983690658643, -0.0005411995177359074], 'target': -5.800871186969663e-05}\n",
      "311 {'train': [0.00027575492754055593, 9.501867241002288e-05, -9.501867241002283e-05, -0.0002757549275405564, -0.0004294983690658643, -0.0005411995177359074, -0.0005999242868511922], 'target': -0.0001683478387208227}\n",
      "312 {'train': [9.501867241002288e-05, -9.501867241002283e-05, -0.0002757549275405564, -0.0004294983690658643, -0.0005411995177359074, -0.0005999242868511922, -0.0005999242868511935], 'target': -0.0002622079061695906}\n",
      "313 {'train': [-9.501867241002283e-05, -0.0002757549275405564, -0.0004294983690658643, -0.0005411995177359074, -0.0005999242868511922, -0.0005999242868511935, -0.0005411995177359073], 'target': -0.0003304012368525733}\n",
      "314 {'train': [-0.0002757549275405564, -0.0004294983690658643, -0.0005411995177359074, -0.0005999242868511922, -0.0005999242868511935, -0.0005411995177359073, -0.0004294983690658643], 'target': -0.0003662525924316445}\n",
      "315 {'train': [-0.0004294983690658643, -0.0005411995177359074, -0.0005999242868511922, -0.0005999242868511935, -0.0005411995177359073, -0.0004294983690658643, -0.0002757549275405564], 'target': -0.00036625259243164526}\n",
      "316 {'train': [-0.0005411995177359074, -0.0005999242868511922, -0.0005999242868511935, -0.0005411995177359073, -0.0004294983690658643, -0.0002757549275405564, -9.50186724100224e-05], 'target': -0.0003304012368525732}\n",
      "317 {'train': [-0.0005999242868511922, -0.0005999242868511935, -0.0005411995177359073, -0.0004294983690658643, -0.0002757549275405564, -9.50186724100224e-05, 9.50186724100228e-05], 'target': -0.0002622079061695908}\n",
      "318 {'train': [-0.0005999242868511935, -0.0005411995177359073, -0.0004294983690658643, -0.0002757549275405564, -9.50186724100224e-05, 9.50186724100228e-05, 0.0002757549275405561], 'target': -0.00016834783872082267}\n",
      "319 {'train': [-0.0005411995177359073, -0.0004294983690658643, -0.0002757549275405564, -9.50186724100224e-05, 9.50186724100228e-05, 0.0002757549275405561, 0.00042949836906586495], 'target': -5.8008711869696373e-05}\n",
      "320 {'train': [-0.0002622079061695908, -0.00016834783872082267, -5.8008711869696373e-05, 5.80087118696966e-05, 0.00016834783872082254, 0.00026220790616959106, 0.00033040123685257295], 'target': 3.541420404466235e-05}\n",
      "321 {'train': [-0.00016834783872082267, -5.8008711869696373e-05, 5.80087118696966e-05, 0.00016834783872082254, 0.00026220790616959106, 0.00033040123685257295, 0.0003662525924316449], 'target': 0.00010277602309682698}\n",
      "322 {'train': [-5.8008711869696373e-05, 5.80087118696966e-05, 0.00016834783872082254, 0.00026220790616959106, 0.00033040123685257295, 0.0003662525924316449, 0.00036625259243164526], 'target': 0.00016007740892561468}\n",
      "323 {'train': [5.80087118696966e-05, 0.00016834783872082254, 0.00026220790616959106, 0.00033040123685257295, 0.0003662525924316449, 0.00036625259243164526, 0.0003304012368525729], 'target': 0.00020170930264387253}\n",
      "324 {'train': [0.00016834783872082254, 0.00026220790616959106, 0.00033040123685257295, 0.0003662525924316449, 0.00036625259243164526, 0.0003304012368525729, 0.000262207906169591], 'target': 0.00022359648442799786}\n",
      "325 {'train': [0.00026220790616959106, 0.00033040123685257295, 0.0003662525924316449, 0.00036625259243164526, 0.0003304012368525729, 0.000262207906169591, 0.00016834783872082243], 'target': 0.0002235964844279981}\n",
      "326 {'train': [0.00033040123685257295, 0.0003662525924316449, 0.00036625259243164526, 0.0003304012368525729, 0.000262207906169591, 0.00016834783872082243, 5.800871186969667e-05], 'target': 0.00020170930264387248}\n",
      "327 {'train': [0.0003662525924316449, 0.00036625259243164526, 0.0003304012368525729, 0.000262207906169591, 0.00016834783872082243, 5.800871186969667e-05, -5.800871186969663e-05], 'target': 0.00016007740892561457}\n",
      "328 {'train': [0.00036625259243164526, 0.0003304012368525729, 0.000262207906169591, 0.00016834783872082243, 5.800871186969667e-05, -5.800871186969663e-05, -0.0001683478387208227], 'target': 0.00010277602309682688}\n",
      "329 {'train': [0.0003304012368525729, 0.000262207906169591, 0.00016834783872082243, 5.800871186969667e-05, -5.800871186969663e-05, -0.0001683478387208227, -0.0002622079061695906], 'target': 3.5414204044662395e-05}\n",
      "330 {'train': [0.000262207906169591, 0.00016834783872082243, 5.800871186969667e-05, -5.800871186969663e-05, -0.0001683478387208227, -0.0002622079061695906, -0.0003304012368525733], 'target': -3.5414204044662375e-05}\n",
      "331 {'train': [0.00016834783872082243, 5.800871186969667e-05, -5.800871186969663e-05, -0.0001683478387208227, -0.0002622079061695906, -0.0003304012368525733, -0.0003662525924316445], 'target': -0.0001027760230968271}\n",
      "332 {'train': [5.800871186969667e-05, -5.800871186969663e-05, -0.0001683478387208227, -0.0002622079061695906, -0.0003304012368525733, -0.0003662525924316445, -0.00036625259243164526], 'target': -0.0001600774089256144}\n",
      "333 {'train': [-5.800871186969663e-05, -0.0001683478387208227, -0.0002622079061695906, -0.0003304012368525733, -0.0003662525924316445, -0.00036625259243164526, -0.0003304012368525732], 'target': -0.00020170930264387275}\n",
      "334 {'train': [-0.0001683478387208227, -0.0002622079061695906, -0.0003304012368525733, -0.0003662525924316445, -0.00036625259243164526, -0.0003304012368525732, -0.0002622079061695908], 'target': -0.0002235964844279976}\n",
      "335 {'train': [-0.0002622079061695906, -0.0003304012368525733, -0.0003662525924316445, -0.00036625259243164526, -0.0003304012368525732, -0.0002622079061695908, -0.00016834783872082267], 'target': -0.0002235964844279981}\n",
      "336 {'train': [-0.0003304012368525733, -0.0003662525924316445, -0.00036625259243164526, -0.0003304012368525732, -0.0002622079061695908, -0.00016834783872082267, -5.8008711869696373e-05], 'target': -0.0002017093026438727}\n",
      "337 {'train': [-0.0003662525924316445, -0.00036625259243164526, -0.0003304012368525732, -0.0002622079061695908, -0.00016834783872082267, -5.8008711869696373e-05, 5.80087118696966e-05], 'target': -0.00016007740892561446}\n",
      "338 {'train': [-0.00036625259243164526, -0.0003304012368525732, -0.0002622079061695908, -0.00016834783872082267, -5.8008711869696373e-05, 5.80087118696966e-05, 0.00016834783872082254], 'target': -0.00010277602309682704}\n",
      "339 {'train': [-0.0003304012368525732, -0.0002622079061695908, -0.00016834783872082267, -5.8008711869696373e-05, 5.80087118696966e-05, 0.00016834783872082254, 0.00026220790616959106], 'target': -3.541420404466221e-05}\n",
      "340 {'train': [-0.00016007740892561446, -0.00010277602309682704, -3.541420404466221e-05, 3.541420404466235e-05, 0.00010277602309682698, 0.00016007740892561468, 0.00020170930264387253], 'target': 2.1620301635626388e-05}\n",
      "341 {'train': [-0.00010277602309682704, -3.541420404466221e-05, 3.541420404466235e-05, 0.00010277602309682698, 0.00016007740892561468, 0.00020170930264387253, 0.00022359648442799786], 'target': 6.274455914528486e-05}\n",
      "342 {'train': [-3.541420404466221e-05, 3.541420404466235e-05, 0.00010277602309682698, 0.00016007740892561468, 0.00020170930264387253, 0.00022359648442799786, 0.0002235964844279981], 'target': 9.772694203875333e-05}\n",
      "343 {'train': [3.541420404466235e-05, 0.00010277602309682698, 0.00016007740892561468, 0.00020170930264387253, 0.00022359648442799786, 0.0002235964844279981, 0.00020170930264387248], 'target': 0.00012314313094182532}\n",
      "344 {'train': [0.00010277602309682698, 0.00016007740892561468, 0.00020170930264387253, 0.00022359648442799786, 0.0002235964844279981, 0.00020170930264387248, 0.00016007740892561457], 'target': 0.0001365052121996674}\n",
      "345 {'train': [0.00016007740892561468, 0.00020170930264387253, 0.00022359648442799786, 0.0002235964844279981, 0.00020170930264387248, 0.00016007740892561457, 0.00010277602309682688], 'target': 0.00013650521219966754}\n",
      "346 {'train': [0.00020170930264387253, 0.00022359648442799786, 0.0002235964844279981, 0.00020170930264387248, 0.00016007740892561457, 0.00010277602309682688, 3.5414204044662395e-05], 'target': 0.00012314313094182532}\n",
      "347 {'train': [0.00022359648442799786, 0.0002235964844279981, 0.00020170930264387248, 0.00016007740892561457, 0.00010277602309682688, 3.5414204044662395e-05, -3.5414204044662375e-05], 'target': 9.772694203875327e-05}\n",
      "348 {'train': [0.0002235964844279981, 0.00020170930264387248, 0.00016007740892561457, 0.00010277602309682688, 3.5414204044662395e-05, -3.5414204044662375e-05, -0.0001027760230968271], 'target': 6.274455914528482e-05}\n",
      "349 {'train': [0.00020170930264387248, 0.00016007740892561457, 0.00010277602309682688, 3.5414204044662395e-05, -3.5414204044662375e-05, -0.0001027760230968271, -0.0001600774089256144], 'target': 2.1620301635626415e-05}\n",
      "350 {'train': [0.00016007740892561457, 0.00010277602309682688, 3.5414204044662395e-05, -3.5414204044662375e-05, -0.0001027760230968271, -0.0001600774089256144, -0.00020170930264387275], 'target': -2.162030163562641e-05}\n",
      "351 {'train': [0.00010277602309682688, 3.5414204044662395e-05, -3.5414204044662375e-05, -0.0001027760230968271, -0.0001600774089256144, -0.00020170930264387275, -0.0002235964844279976], 'target': -6.274455914528496e-05}\n",
      "352 {'train': [3.5414204044662395e-05, -3.5414204044662375e-05, -0.0001027760230968271, -0.0001600774089256144, -0.00020170930264387275, -0.0002235964844279976, -0.0002235964844279981], 'target': -9.772694203875315e-05}\n",
      "353 {'train': [-3.5414204044662375e-05, -0.0001027760230968271, -0.0001600774089256144, -0.00020170930264387275, -0.0002235964844279976, -0.0002235964844279981, -0.0002017093026438727], 'target': -0.00012314313094182548}\n",
      "354 {'train': [-0.0001027760230968271, -0.0001600774089256144, -0.00020170930264387275, -0.0002235964844279976, -0.0002235964844279981, -0.0002017093026438727, -0.00016007740892561446], 'target': -0.00013650521219966724}\n",
      "355 {'train': [-0.0001600774089256144, -0.00020170930264387275, -0.0002235964844279976, -0.0002235964844279981, -0.0002017093026438727, -0.00016007740892561446, -0.00010277602309682704], 'target': -0.00013650521219966754}\n",
      "356 {'train': [-0.00020170930264387275, -0.0002235964844279976, -0.0002235964844279981, -0.0002017093026438727, -0.00016007740892561446, -0.00010277602309682704, -3.541420404466221e-05], 'target': -0.00012314313094182545}\n",
      "357 {'train': [-0.0002235964844279976, -0.0002235964844279981, -0.0002017093026438727, -0.00016007740892561446, -0.00010277602309682704, -3.541420404466221e-05, 3.541420404466235e-05], 'target': -9.772694203875319e-05}\n",
      "358 {'train': [-0.0002235964844279981, -0.0002017093026438727, -0.00016007740892561446, -0.00010277602309682704, -3.541420404466221e-05, 3.541420404466235e-05, 0.00010277602309682698], 'target': -6.274455914528492e-05}\n",
      "359 {'train': [-0.0002017093026438727, -0.00016007740892561446, -0.00010277602309682704, -3.541420404466221e-05, 3.541420404466235e-05, 0.00010277602309682698, 0.00016007740892561468], 'target': -2.1620301635626307e-05}\n",
      "360 {'train': [-9.772694203875319e-05, -6.274455914528492e-05, -2.1620301635626307e-05, 2.1620301635626388e-05, 6.274455914528486e-05, 9.772694203875333e-05, 0.00012314313094182532], 'target': 1.3199151454200782e-05}\n",
      "361 {'train': [-6.274455914528492e-05, -2.1620301635626307e-05, 2.1620301635626388e-05, 6.274455914528486e-05, 9.772694203875333e-05, 0.00012314313094182532, 0.0001365052121996674], 'target': 3.83054294543694e-05}\n",
      "362 {'train': [-2.1620301635626307e-05, 2.1620301635626388e-05, 6.274455914528486e-05, 9.772694203875333e-05, 0.00012314313094182532, 0.0001365052121996674, 0.00013650521219966754], 'target': 5.966210512992389e-05}\n",
      "363 {'train': [2.1620301635626388e-05, 6.274455914528486e-05, 9.772694203875333e-05, 0.00012314313094182532, 0.0001365052121996674, 0.00013650521219966754, 0.00012314313094182532], 'target': 7.517863826503179e-05}\n",
      "364 {'train': [6.274455914528486e-05, 9.772694203875333e-05, 0.00012314313094182532, 0.0001365052121996674, 0.00013650521219966754, 0.00012314313094182532, 9.772694203875327e-05], 'target': 8.333616248638549e-05}\n",
      "365 {'train': [9.772694203875333e-05, 0.00012314313094182532, 0.0001365052121996674, 0.00013650521219966754, 0.00012314313094182532, 9.772694203875327e-05, 6.274455914528482e-05], 'target': 8.333616248638558e-05}\n",
      "366 {'train': [0.00012314313094182532, 0.0001365052121996674, 0.00013650521219966754, 0.00012314313094182532, 9.772694203875327e-05, 6.274455914528482e-05, 2.1620301635626415e-05], 'target': 7.517863826503179e-05}\n",
      "367 {'train': [0.0001365052121996674, 0.00013650521219966754, 0.00012314313094182532, 9.772694203875327e-05, 6.274455914528482e-05, 2.1620301635626415e-05, -2.162030163562641e-05], 'target': 5.966210512992384e-05}\n",
      "368 {'train': [0.00013650521219966754, 0.00012314313094182532, 9.772694203875327e-05, 6.274455914528482e-05, 2.1620301635626415e-05, -2.162030163562641e-05, -6.274455914528496e-05], 'target': 3.830542945436937e-05}\n",
      "369 {'train': [0.00012314313094182532, 9.772694203875327e-05, 6.274455914528482e-05, 2.1620301635626415e-05, -2.162030163562641e-05, -6.274455914528496e-05, -9.772694203875315e-05], 'target': 1.31991514542008e-05}\n",
      "370 {'train': [9.772694203875327e-05, 6.274455914528482e-05, 2.1620301635626415e-05, -2.162030163562641e-05, -6.274455914528496e-05, -9.772694203875315e-05, -0.00012314313094182548], 'target': -1.3199151454200794e-05}\n",
      "371 {'train': [6.274455914528482e-05, 2.1620301635626415e-05, -2.162030163562641e-05, -6.274455914528496e-05, -9.772694203875315e-05, -0.00012314313094182548, -0.00013650521219966724], 'target': -3.830542945436944e-05}\n",
      "372 {'train': [2.1620301635626415e-05, -2.162030163562641e-05, -6.274455914528496e-05, -9.772694203875315e-05, -0.00012314313094182548, -0.00013650521219966724, -0.00013650521219966754], 'target': -5.9662105129923774e-05}\n",
      "373 {'train': [-2.162030163562641e-05, -6.274455914528496e-05, -9.772694203875315e-05, -0.00012314313094182548, -0.00013650521219966724, -0.00013650521219966754, -0.00012314313094182545], 'target': -7.517863826503187e-05}\n",
      "374 {'train': [-6.274455914528496e-05, -9.772694203875315e-05, -0.00012314313094182548, -0.00013650521219966724, -0.00013650521219966754, -0.00012314313094182545, -9.772694203875319e-05], 'target': -8.333616248638539e-05}\n",
      "375 {'train': [-9.772694203875315e-05, -0.00012314313094182548, -0.00013650521219966724, -0.00013650521219966754, -0.00012314313094182545, -9.772694203875319e-05, -6.274455914528492e-05], 'target': -8.333616248638558e-05}\n",
      "376 {'train': [-0.00012314313094182548, -0.00013650521219966724, -0.00013650521219966754, -0.00012314313094182545, -9.772694203875319e-05, -6.274455914528492e-05, -2.1620301635626307e-05], 'target': -7.517863826503187e-05}\n",
      "377 {'train': [-0.00013650521219966724, -0.00013650521219966754, -0.00012314313094182545, -9.772694203875319e-05, -6.274455914528492e-05, -2.1620301635626307e-05, 2.1620301635626388e-05], 'target': -5.966210512992378e-05}\n",
      "378 {'train': [-0.00013650521219966754, -0.00012314313094182545, -9.772694203875319e-05, -6.274455914528492e-05, -2.1620301635626307e-05, 2.1620301635626388e-05, 6.274455914528486e-05], 'target': -3.830542945436943e-05}\n",
      "379 {'train': [-0.00012314313094182545, -9.772694203875319e-05, -6.274455914528492e-05, -2.1620301635626307e-05, 2.1620301635626388e-05, 6.274455914528486e-05, 9.772694203875333e-05], 'target': -1.3199151454200735e-05}\n",
      "380 {'train': [-5.966210512992378e-05, -3.830542945436943e-05, -1.3199151454200735e-05, 1.3199151454200782e-05, 3.83054294543694e-05, 5.966210512992389e-05, 7.517863826503179e-05], 'target': 8.05805589797376e-06}\n",
      "381 {'train': [-3.830542945436943e-05, -1.3199151454200735e-05, 1.3199151454200782e-05, 3.83054294543694e-05, 5.966210512992389e-05, 7.517863826503179e-05, 8.333616248638549e-05], 'target': 2.338538903885084e-05}\n",
      "382 {'train': [-1.3199151454200735e-05, 1.3199151454200782e-05, 3.83054294543694e-05, 5.966210512992389e-05, 7.517863826503179e-05, 8.333616248638549e-05, 8.333616248638558e-05], 'target': 3.642359736501894e-05}\n",
      "383 {'train': [1.3199151454200782e-05, 3.83054294543694e-05, 5.966210512992389e-05, 7.517863826503179e-05, 8.333616248638549e-05, 8.333616248638558e-05, 7.517863826503179e-05], 'target': 4.589641020297365e-05}\n",
      "384 {'train': [3.83054294543694e-05, 5.966210512992389e-05, 7.517863826503179e-05, 8.333616248638549e-05, 8.333616248638558e-05, 7.517863826503179e-05, 5.966210512992384e-05], 'target': 5.0876562631167966e-05}\n",
      "385 {'train': [5.966210512992389e-05, 7.517863826503179e-05, 8.333616248638549e-05, 8.333616248638558e-05, 7.517863826503179e-05, 5.966210512992384e-05, 3.830542945436937e-05], 'target': 5.087656263116802e-05}\n",
      "386 {'train': [7.517863826503179e-05, 8.333616248638549e-05, 8.333616248638558e-05, 7.517863826503179e-05, 5.966210512992384e-05, 3.830542945436937e-05, 1.31991514542008e-05], 'target': 4.589641020297363e-05}\n",
      "387 {'train': [8.333616248638549e-05, 8.333616248638558e-05, 7.517863826503179e-05, 5.966210512992384e-05, 3.830542945436937e-05, 1.31991514542008e-05, -1.3199151454200794e-05], 'target': 3.6423597365018935e-05}\n",
      "388 {'train': [8.333616248638558e-05, 7.517863826503179e-05, 5.966210512992384e-05, 3.830542945436937e-05, 1.31991514542008e-05, -1.3199151454200794e-05, -3.830542945436944e-05], 'target': 2.3385389038850824e-05}\n",
      "389 {'train': [7.517863826503179e-05, 5.966210512992384e-05, 3.830542945436937e-05, 1.31991514542008e-05, -1.3199151454200794e-05, -3.830542945436944e-05, -5.9662105129923774e-05], 'target': 8.058055897973772e-06}\n",
      "390 {'train': [5.966210512992384e-05, 3.830542945436937e-05, 1.31991514542008e-05, -1.3199151454200794e-05, -3.830542945436944e-05, -5.9662105129923774e-05, -7.517863826503187e-05], 'target': -8.058055897973766e-06}\n",
      "391 {'train': [3.830542945436937e-05, 1.31991514542008e-05, -1.3199151454200794e-05, -3.830542945436944e-05, -5.9662105129923774e-05, -7.517863826503187e-05, -8.333616248638539e-05], 'target': -2.3385389038850865e-05}\n",
      "392 {'train': [1.31991514542008e-05, -1.3199151454200794e-05, -3.830542945436944e-05, -5.9662105129923774e-05, -7.517863826503187e-05, -8.333616248638539e-05, -8.333616248638558e-05], 'target': -3.642359736501888e-05}\n",
      "393 {'train': [-1.3199151454200794e-05, -3.830542945436944e-05, -5.9662105129923774e-05, -7.517863826503187e-05, -8.333616248638539e-05, -8.333616248638558e-05, -7.517863826503187e-05], 'target': -4.5896410202973696e-05}\n",
      "394 {'train': [-3.830542945436944e-05, -5.9662105129923774e-05, -7.517863826503187e-05, -8.333616248638539e-05, -8.333616248638558e-05, -7.517863826503187e-05, -5.966210512992378e-05], 'target': -5.0876562631167925e-05}\n",
      "395 {'train': [-5.9662105129923774e-05, -7.517863826503187e-05, -8.333616248638539e-05, -8.333616248638558e-05, -7.517863826503187e-05, -5.966210512992378e-05, -3.830542945436943e-05], 'target': -5.087656263116802e-05}\n",
      "396 {'train': [-7.517863826503187e-05, -8.333616248638539e-05, -8.333616248638558e-05, -7.517863826503187e-05, -5.966210512992378e-05, -3.830542945436943e-05, -1.3199151454200735e-05], 'target': -4.589641020297369e-05}\n",
      "397 {'train': [-8.333616248638539e-05, -8.333616248638558e-05, -7.517863826503187e-05, -5.966210512992378e-05, -3.830542945436943e-05, -1.3199151454200735e-05, 1.3199151454200782e-05], 'target': -3.6423597365018894e-05}\n",
      "398 {'train': [-8.333616248638558e-05, -7.517863826503187e-05, -5.966210512992378e-05, -3.830542945436943e-05, -1.3199151454200735e-05, 1.3199151454200782e-05, 3.83054294543694e-05], 'target': -2.3385389038850858e-05}\n",
      "399 {'train': [-7.517863826503187e-05, -5.966210512992378e-05, -3.830542945436943e-05, -1.3199151454200735e-05, 1.3199151454200782e-05, 3.83054294543694e-05, 5.966210512992389e-05], 'target': -8.05805589797373e-06}\n"
     ]
    }
   ],
   "source": [
    "x, t, solu = gen_analytical_cell_averaged(delta_x=1/10, delta_t=1/20, xmin=0, tmin=0, xmax=2, tmax=1, analytical_eq=heat_equ_analytical_solu)\n",
    "print(solu[0])\n",
    "pairs = get_trainingset_all(solu, 3, 3, recursive_padding)\n",
    "for i, p in enumerate(pairs):\n",
    "    print(i, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============\n",
    "# FullconnectedResNet\n",
    "# ==============\n",
    "\n",
    "class FullconnectedResNet(nn.Module):\n",
    "    def __init__(self, i, o, layer_data, hasDropout, p, num, weight=1):\n",
    "        super(FullconnectedResNet, self).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module(\"linear_1\", nn.Linear(i, layer_data[0]))\n",
    "        if hasDropout:\n",
    "            self.layers.add_module(\"dropout_1\", nn.Dropout(p=p))\n",
    "        self.layers.add_module(\"relu_1\", nn.ReLU())\n",
    "        for index in range(len(layer_data)-1):\n",
    "            self.layers.add_module(\"linear_\"+str(index+2), nn.Linear(layer_data[index], layer_data[index+1]))\n",
    "            if hasDropout:\n",
    "                self.layers.add_module(\"dropout_2\", nn.Dropout(p=p))\n",
    "            self.layers.add_module(\"relu_\"+str(index+2), nn.ReLU())\n",
    "        self.layers.add_module(\"linear_3\"+str(len(layer_data)+1), nn.Linear(layer_data[len(layer_data)-1], o))\n",
    "        if hasDropout:\n",
    "            self.layers.add_module(\"dropout_3\", nn.Dropout(p=p))\n",
    "        self.layers.add_module(\"relu_\"+str(len(layer_data)+1), nn.ReLU())\n",
    "        self.num = num\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "        return output + self.weight * x[self.num], x[self.num], output\n",
    "\n",
    "    def load_model(self, save_path):\n",
    "        self.load_state_dict(torch.load(save_path))\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "        torch.save(self.state_dict(), save_path)\n",
    "        \n",
    "# ==========\n",
    "# BidirectionRNN\n",
    "# ==========\n",
    "\n",
    "# class BidirectionRNN(nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printer_of_nn(counter, train, target, loss, output, res, pure, TorE):\n",
    "    print(\"=\"*60)\n",
    "    print(\"This is\", counter+1, \"times of iteration\")\n",
    "    if TorE:\n",
    "        print(\"Training...\")\n",
    "    else:\n",
    "        print(\"Evaluating...\")\n",
    "    print(\"The input is:\", train)\n",
    "    print(\"The output is:\", output)\n",
    "    print(\"The correct solution is:\", target)\n",
    "    print(\"The pure output is:\", pure)\n",
    "    print(\"The res input is:\", res)\n",
    "    if TorE:\n",
    "        print(\"The training loss is:\", loss)\n",
    "    else:\n",
    "        print(\"The evaluation loss is:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_config_generator(\n",
    "    delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq, gen_analytical_method, allTheTime,\n",
    "    num1, num2, padding, train_takeAll, eval_takeAll, size1, size2,\n",
    "    layer_data, nn_model, optim, learning_rate, iteration, hasDropout, p, weight,\n",
    "    loading, load_path,\n",
    "    x_file, t_file, actual_solu_file, config_file, train_loss_file, model_file, eval_loss_file, prediction_file\n",
    "):\n",
    "    data_setting = {'delta_x': delta_x,'delta_t': delta_t,'xmin': xmin, 'tmin': tmin, 'xmax': xmax, 'tmax': tmax, 'PDE': str(analytical_eq), 'method': str(gen_analytical_method), 'allTheTime': allTheTime}\n",
    "    pairs_setting = {'left': num1,'right': num2, 'dim': num1+num2+1, 'padding': str(padding), 'train_takeAll': train_takeAll, 'eval_takeAll': eval_takeAll, 'size_train': size1, 'size_eval': size2}\n",
    "    nn_setting = {'model': str(nn_model), 'layer_data': layer_data, 'input': num1+num2+1, 'output': 1, 'hasDropout': hasDropout, 'dropout_rate': p, 'res_weight': weight}\n",
    "    optim_setting = {'optimizer': str(optim), 'learning_rate': learning_rate}\n",
    "    model_setting = {'nn_model': nn_setting, 'optim': optim_setting, 'iteration': iteration, 'is_loading': loading, 'loading_path': load_path}\n",
    "    x_file_setting = {'path': x_file, 'explain': 'x'}\n",
    "    t_file_setting = {'path': t_file, 'explain': 't'}\n",
    "    actual_solu_file_setting = {'path': actual_solu_file, 'explain': 'the actual/correct/analytical solutions'}\n",
    "    config_file_setting = {'path': config_file, 'explain': 'the current file, store all the configurations of each experiment'}\n",
    "    train_loss_file_setting = {'path': train_loss_file, 'explain': 'the training loss'}\n",
    "    model_file_setting = {'path': model_file, 'explain': 'the saved model, can be loaded for further training'}\n",
    "    eval_loss_file_setting = {'path': eval_loss_file, 'explain': 'the evaluation loss'}\n",
    "    prediction_file_setting = {'path': prediction_file, 'explain': 'final predictions'}\n",
    "    files = {\n",
    "        'x_file_setting': x_file_setting,\n",
    "        't_file_setting': t_file_setting,\n",
    "        'actual_solu_file_setting': actual_solu_file_setting,\n",
    "        'config_file_setting': config_file_setting,\n",
    "        'train_loss_file_setting': train_loss_file_setting,\n",
    "        'model_file_setting': model_file_setting,\n",
    "        'eval_loss_file_setting': eval_loss_file_setting,\n",
    "        'prediction_file_setting': prediction_file_setting\n",
    "    }\n",
    "    json_data = {\n",
    "        'data_setting': data_setting,\n",
    "        'pairs_setting': pairs_setting,\n",
    "        'model_setting': model_setting,\n",
    "        'files_setting': files\n",
    "    }\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_the_model(\n",
    "    folder_name,\n",
    "    delta_x=1/20, delta_t=1/10, xmin=0, tmin=0, xmax=1, tmax=2, analytical_eq=heat_equ_analytical_solu, gen_analytical_method=gen_analytical_cell_averaged, allTheTime=False,\n",
    "    num1=3, num2=3, padding=recursive_padding, train_takeAll=True, eval_takeAll=True, size1=0, size2=0,\n",
    "    layer_data=[6, 6], nn_model=FullconnectedResNet, optim=optim.Adam, learning_rate=0.001, iteration=100, hasDropout=False, p=0.2, weight=1,\n",
    "    loading=False, load_path=\"\",\n",
    "    config_file=\"config.json\", actual_solu_file='actual solution.csv', x_file='x.csv', t_file='t.csv',\n",
    "    train_loss_file='training loss.txt', model_file='model',\n",
    "    eval_loss_file='evaluation loss.txt', prediction_file='prediction.csv',\n",
    "    doEval=True, printTraining=False, printEval=False\n",
    "):\n",
    "    # ====================\n",
    "    # Configuration and parameters\n",
    "    # ====================\n",
    "    json_data = json_config_generator(\n",
    "    delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq, gen_analytical_method, allTheTime,\n",
    "    num1, num2, padding, train_takeAll, eval_takeAll, size1, size2,\n",
    "    layer_data, nn_model, optim, learning_rate, iteration, hasDropout, p, weight,\n",
    "    loading, load_path,\n",
    "    x_file, t_file, actual_solu_file, config_file, train_loss_file, model_file, eval_loss_file, prediction_file)\n",
    "    # ==================\n",
    "    # Creating pathes and config\n",
    "    # ==================\n",
    "    x_file = folder_name+\"/\"+x_file\n",
    "    t_file = folder_name+\"/\"+t_file\n",
    "    actual_solu_file = folder_name+\"/\"+actual_solu_file\n",
    "    config_file = folder_name+\"/\"+config_file\n",
    "    train_loss_file = folder_name+\"/\"+train_loss_file\n",
    "    model_file = folder_name+\"/\"+model_file\n",
    "    eval_loss_file = folder_name+\"/\"+eval_loss_file\n",
    "    prediction_file = folder_name+\"/\"+prediction_file\n",
    "    # ==========\n",
    "    # Creating folder\n",
    "    # ==========\n",
    "    mkdir(folder_name)\n",
    "    # ===========\n",
    "    # Prepare the data\n",
    "    # ===========\n",
    "    x, t, solu = gen_analytical_method(delta_x=delta_x, delta_t=delta_t, xmin=xmin, tmin=tmin, xmax=xmax, tmax=tmax, analytical_eq=analytical_eq)\n",
    "    actual_solu = solu\n",
    "    if not allTheTime:\n",
    "        solu = solu[:2]\n",
    "    # ===========\n",
    "    # Prepare the pairs\n",
    "    # ===========\n",
    "    if train_takeAll:\n",
    "        pairs = get_trainingset_all(solu, num1, num2, padding)\n",
    "    else:\n",
    "        pairs = get_trainingset_random(solu, num1, num2, padding, size)\n",
    "    # =================\n",
    "    # Set up model & optimizer\n",
    "    # =================\n",
    "    model = nn_model(i=num1+num2+1, o=1, layer_data=layer_data, hasDropout=hasDropout, p=p, num=num1, weight=weight)\n",
    "    if loading:\n",
    "        model.load_model(load_path)\n",
    "    optimizer = optim(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    # ===========\n",
    "    # Train the model\n",
    "    # ===========\n",
    "    model.train()\n",
    "    list_of_losses_t = []\n",
    "    list_of_outputs = []\n",
    "    counter = 0\n",
    "    for itera in range(iteration):\n",
    "        for pair in pairs:\n",
    "            train = torch.FloatTensor(pair[\"train\"])\n",
    "            target = torch.FloatTensor([pair[\"target\"]])\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            output, res, pure = model(train)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            list_of_losses_t.append(loss.item())\n",
    "            list_of_outputs.append(output)\n",
    "        if printTraining:\n",
    "            printer_of_nn(counter=counter, train=train, target=target, loss=loss, output=output, res=res, pure=pure, TorE=True)\n",
    "        counter += 1\n",
    "    # for parameter in model.parameters():\n",
    "    #     print(parameter)\n",
    "    # =====\n",
    "    # Saving\n",
    "    # =====\n",
    "    save_csv(actual_solu_file, actual_solu)\n",
    "    save_csv(x_file, [x])\n",
    "    save_csv(t_file, [t])\n",
    "    save_json(config_file, json_data)\n",
    "    save_list(train_loss_file, list_of_losses_t)\n",
    "    model.save_model(model_file)\n",
    "    # ==========\n",
    "    # Do we do eval?\n",
    "    # ==========\n",
    "    if not doEval:\n",
    "        return 0\n",
    "    # ===========\n",
    "    # Prepare the pairs\n",
    "    # ===========\n",
    "    if eval_takeAll:\n",
    "        eval_pairs = get_testingset_all(actual_solu, num1, num2, padding)\n",
    "    else:\n",
    "        eval_pairs = get_testingset_random(actual_solu, num1, num2, padding, size)\n",
    "    # =============\n",
    "    # Evaluate the model\n",
    "    # =============\n",
    "    model.eval()\n",
    "    list_of_losses_e = []\n",
    "    prediction = [solu[0]]\n",
    "    counter = 0\n",
    "    for j in range(len(eval_pairs)):\n",
    "        pairs_t = eval_pairs[j]\n",
    "        prediction_t = []\n",
    "        for pair in pairs_t:\n",
    "            train = torch.FloatTensor(pair[\"train\"])\n",
    "            target = torch.FloatTensor([pair[\"target\"]])\n",
    "            output, res, pure = model(train)\n",
    "            loss = criterion(output, target)\n",
    "            list_of_losses_e.append(loss.item())\n",
    "            prediction_t.append(output.item())\n",
    "            if printTraining:\n",
    "                printer_of_nn(counter=counter, train=train, target=target, loss=loss, output=output, res=res, pure=pure, TorE=False)\n",
    "        counter += 1\n",
    "        prediction.append(prediction_t)\n",
    "    # =====\n",
    "    # Saving\n",
    "    # =====\n",
    "    save_list(eval_loss_file, list_of_losses_e)\n",
    "    save_csv(prediction_file, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "This is 1 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 2 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.2523], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.4081], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.1207, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 3 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.2207], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.3765], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0997, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 4 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.2034], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.3592], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0891, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 5 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.2318], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.3876], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.1069, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 6 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.2128], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.3686], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0948, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 7 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 8 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.1548], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.3106], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0624, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 9 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 10 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.1551], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.3109], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0626, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 11 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0753], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.2311], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0290, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 12 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 13 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0536], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.2094], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0221, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 14 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 15 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0311], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1868], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0159, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 16 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 17 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0055], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1613], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0101, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 18 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0167], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1391], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 19 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0361], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1197], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 20 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 21 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0569], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0988], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 22 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0063], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1621], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0103, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 23 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 24 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0612], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0946], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 25 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0576], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.2134], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0233, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 26 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 27 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0675], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0883], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 28 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 29 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0381], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1177], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 30 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1120], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0438], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 31 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1078], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0480], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 32 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1109], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0449], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 33 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0568], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0990], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 34 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0578], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0980], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 35 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0761], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0796], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 36 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0786], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0772], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 37 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 38 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1460], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0098], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 39 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1387], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0171], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 40 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 41 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "This is 42 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 43 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1288], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0270], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 44 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 45 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1318], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0240], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 46 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1192], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0366], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 47 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1157], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0401], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 48 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1074], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0484], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 49 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0559], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0998], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 50 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1233], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0325], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 51 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1329], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0229], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 52 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 53 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 54 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 55 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 56 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1220], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0338], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 57 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 58 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1308], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0250], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 59 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1411], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0147], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 60 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1203], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0355], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 61 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1311], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0247], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 62 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1106], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0452], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 63 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 64 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 65 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1204], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0354], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 66 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1260], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0297], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 67 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1186], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0372], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 68 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1183], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0375], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 69 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0306], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1864], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0158, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 70 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1348], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0210], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 71 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1339], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0219], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 72 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1446], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0112], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 73 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0323], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1881], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0162, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 74 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 75 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0207], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1765], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 76 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1194], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0364], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 77 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0806], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0752], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 78 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1441], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0117], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 79 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1509], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0049], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 80 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0680], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0878], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 81 times of iteration\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0430], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1988], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0191, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 82 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0624], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0934], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 83 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0841], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0717], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 84 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 85 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1547], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0011], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 86 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 87 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1333], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0225], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 88 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 89 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0026], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1584], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 90 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 91 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0999], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0559], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(2.2883e-05, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 92 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 93 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0980], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0578], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(8.3595e-06, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 94 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0692], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.2250], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0270, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 95 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 96 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 97 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 98 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 99 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 100 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 101 times of iteration\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1544], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0014], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 102 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1439], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0119], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 103 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1335], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0223], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 104 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1177], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0381], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 105 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1196], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0361], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 106 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1026], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0532], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(5.5866e-05, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 107 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1051], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0507], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 108 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1130], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0428], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 109 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1110], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0448], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 110 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 111 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 112 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0660], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0897], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 113 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 114 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 115 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 116 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1267], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0291], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 117 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1204], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0354], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 118 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0912], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.2470], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0347, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 119 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0665], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0893], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 120 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0911], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.2469], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0347, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 121 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1195], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0362], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 122 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1151], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0407], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 123 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1110], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0447], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is: tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 124 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1124], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0434], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 125 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 126 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0451], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.2009], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0197, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 127 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0386], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1944], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0179, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 128 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1265], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0293], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 129 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1222], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0336], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 130 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 131 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1164], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0394], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 132 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 133 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0299], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1857], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0156, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 134 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1303], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0255], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 135 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1264], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0294], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 136 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0650], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0908], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 137 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0725], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0833], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 138 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0286], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1844], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0153, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 139 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 140 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1377], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0181], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 141 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0754], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0804], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 142 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1333], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0225], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 143 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1238], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0320], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0008, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "This is 144 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0852], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.2409], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0325, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 145 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0292], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1849], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0154, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 146 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1288], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0270], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 147 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0167], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1725], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0125, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 148 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1496], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0062], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 149 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 150 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1343], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0214], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 151 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 152 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1257], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0301], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 153 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1230], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0328], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 154 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1187], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0371], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 155 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 156 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0818], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.2376], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0313, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 157 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1294], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0264], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 158 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 159 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0797], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0761], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 160 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0264], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1822], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 161 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1440], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0118], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 162 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 163 times of iteration\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1332], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0226], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 164 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0267], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1825], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 165 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0783], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.2341], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0301, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 166 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1379], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0179], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 167 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0897], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0661], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(2.9313e-05, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 168 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 169 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1475], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0083], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 170 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0180], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1738], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0128, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 171 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 172 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0025], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1583], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 173 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1002], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0556], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(2.5439e-05, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 174 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 175 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 176 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1425], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0133], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 177 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 178 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1396], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0162], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 179 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 180 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1342], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0216], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 181 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 182 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1299], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0259], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 183 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 184 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0236], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1322], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 185 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0087], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1645], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0108, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 186 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0486], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1072], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 187 times of iteration\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0089], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1469], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 188 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1188], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0369], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 189 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 190 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1056], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0502], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 191 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 192 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 193 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1000], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 194 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 195 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 196 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1109], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0449], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 197 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1505], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0053], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 198 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0926], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0632], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(6.1349e-06, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 199 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0359], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1917], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0172, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 200 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 201 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1186], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0372], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 202 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0727], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0831], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 203 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1313], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0245], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 204 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0198], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1360], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 205 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 206 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 207 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 208 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 209 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 210 times of iteration\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 211 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 212 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1518], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0040], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 213 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 214 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 215 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 216 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 217 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0752], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0806], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 218 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 219 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 220 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 221 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 222 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 223 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 224 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 225 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 226 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 227 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 228 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 229 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1478], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0080], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 230 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 231 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1273], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0285], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 232 times of iteration\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0431], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1127], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 233 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 234 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1408], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0150], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 235 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0602], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0956], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 236 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 237 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 238 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 239 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1433], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0125], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 240 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 241 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 242 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 243 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 244 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 245 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 246 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 247 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 248 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1220], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0338], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 249 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1107], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0451], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 250 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 251 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 252 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 253 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 254 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 255 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is: tensor([0.0578], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.2136], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0234, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 256 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1519], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0039], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 257 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 258 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 259 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 260 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 261 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 262 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 263 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 264 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1306], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0252], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 265 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0471], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1087], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 266 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 267 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 268 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0085], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1473], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 269 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 270 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0827], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0731], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 271 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 272 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 273 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 274 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 275 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0853], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0705], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(9.6828e-05, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 276 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1375], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0183], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 277 times of iteration\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 278 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 279 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1175], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0383], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 280 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 281 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 282 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1006], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0551], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(3.0601e-05, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 283 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0892], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0666], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(3.5426e-05, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 284 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0440], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1998], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0194, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 285 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 286 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1353], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0205], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 287 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 288 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 289 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1120], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0438], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 290 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 291 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 292 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 293 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0630], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0928], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 294 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 295 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 296 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 297 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 298 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 299 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 300 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.0390], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1948], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0180, grad_fn=<MseLossBackward>)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 301 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 302 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 303 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 304 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 305 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 306 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 307 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 308 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 309 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 310 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 311 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 312 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 313 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 314 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 315 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0160], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1398], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 316 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 317 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 318 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 319 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 320 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "This is 321 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 322 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0954], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0604], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(6.4883e-08, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 323 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 324 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 325 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 326 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 327 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1016], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0542], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(4.2038e-05, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 328 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 329 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 330 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 331 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 332 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 333 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 334 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1508], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0050], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 335 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 336 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 337 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 338 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 339 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0991], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0567], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(1.5534e-05, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 340 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 341 times of iteration\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 342 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 343 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0876], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0682], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(5.6150e-05, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 344 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 345 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 346 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 347 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 348 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.0167], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.1391], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 349 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1287], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.0271], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 350 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 351 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 352 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 353 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================\n",
      "This is 354 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "============================================================"
     ]
    }
   ],
   "source": [
    "training_the_model(\n",
    "    delta_x=1/10, delta_t=1/20, xmin=0, tmin=0, xmax=2, tmax=1, learning_rate=0.001, layer_data=[6], hasDropout=True, p=0.2,\n",
    "    folder_name=\"10 10 control group [do dropout p=0.2]\", iteration=1000, printTraining=True, printEval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
