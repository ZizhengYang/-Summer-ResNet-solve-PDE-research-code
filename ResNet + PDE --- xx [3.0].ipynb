{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate as integrate\n",
    "import scipy.special as special\n",
    "\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim\n",
    "\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import exp,arange\n",
    "from pylab import meshgrid,cm,imshow,contour,clabel,colorbar,axis,title,show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sin(\\pi x) e^{-\\pi^2 t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_equ_analytical_solu(x, t):\n",
    "    return np.sin(np.pi * x) * np.exp(-np.power(np.pi, 2) * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============\n",
    "# save and load: json\n",
    "# ============\n",
    "\n",
    "def save_json(save_path, data):\n",
    "    assert save_path.split('.')[-1] == 'json'\n",
    "    with open(save_path,'w+') as file:\n",
    "        json.dump(data,file)\n",
    "\n",
    "def load_json(file_path):\n",
    "    assert file_path.split('.')[-1] == 'json'\n",
    "    with open(file_path,'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# ============\n",
    "# save and load: csv\n",
    "# ============\n",
    "\n",
    "def save_csv(save_path, data):\n",
    "    with open(save_path, \"w+\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)\n",
    "\n",
    "def load_csv(file_path):\n",
    "    string = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            if i%2==0:\n",
    "                string.append(row)\n",
    "    ret = []\n",
    "    for p in string:\n",
    "        ret.append(np.array([float(i) for i in p]))\n",
    "    return ret\n",
    "\n",
    "# ==============\n",
    "# save and load: normal\n",
    "# ==============\n",
    "\n",
    "def save_list(save_path, data):\n",
    "    file = open(save_path, 'w+')\n",
    "    for value in data:\n",
    "        file.write(str(value)+\" \")\n",
    "    file.close()\n",
    "\n",
    "# def load_list(file_path):\n",
    "\n",
    "# ==========\n",
    "# make directory\n",
    "# ==========\n",
    "\n",
    "def mkdir(folder_name):\n",
    "    folder = os.path.exists(folder_name)\n",
    "    if not folder:\n",
    "        os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======\n",
    "# OPTIONS\n",
    "# =======\n",
    "\n",
    "# ===========\n",
    "# padding method\n",
    "# ===========\n",
    "\n",
    "def padding(starting_padding, original, end_padding):\n",
    "    return np.hstack((starting_padding, original, end_padding)).tolist()\n",
    "\n",
    "def zero_padding(original, num1, num2):\n",
    "    starting_padding = [0 for i in range(num1)]\n",
    "    end_padding = [0 for i in range(num2)]\n",
    "    return padding(starting_padding, original, end_padding)\n",
    "\n",
    "def border_padding(original, num1, num2):\n",
    "    starting = original[0]\n",
    "    ending = original[len(original)-1]\n",
    "    starting_padding = [starting for i in range(num1)]\n",
    "    end_padding = [ending for i in range(num2)]\n",
    "    return padding(starting_padding, original, end_padding)\n",
    "\n",
    "def recursive_padding(original, num1, num2):\n",
    "    starting_padding = original[-num1:]\n",
    "    end_padding = original[:num2]\n",
    "    return padding(starting_padding, original, end_padding)\n",
    "\n",
    "def random_padding(original, num1, num2):\n",
    "    max_value = np.max(original)\n",
    "    min_value = np.min(original)\n",
    "    starting_padding = [random.randint(min_value, max_value) for i in range(num1)]\n",
    "    end_padding = [random.randint(min_value, max_value) for i in range(num2)]\n",
    "    return padding(starting_padding, original, end_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======\n",
    "# OPTIONS\n",
    "# =======\n",
    "\n",
    "# ==================\n",
    "# analytical solution generator\n",
    "# ==================\n",
    "\n",
    "def gen_analytical(delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq):\n",
    "    x = arange(xmin, xmax+delta_x, delta_x)\n",
    "    t = arange(tmin, tmax+delta_t, delta_t)\n",
    "    X,T = meshgrid(x, t) # grid of point\n",
    "    solu = analytical_eq(X, T) # evaluation of the function on the grid\n",
    "    return x, t, solu\n",
    "\n",
    "def gen_analytical_averaged(delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq):\n",
    "    x = arange(xmin, xmax+delta_x, delta_x)\n",
    "    t = arange(tmin, tmax+delta_t, delta_t)\n",
    "    X,T = meshgrid(x, t) # grid of point\n",
    "    Z = analytical_eq(X, T) # evaluation of the function on the grid\n",
    "    solu = []\n",
    "    for zz in Z:\n",
    "        solu_t = []\n",
    "        for j in range(len(zz)-1):\n",
    "            value = (1/2) * (zz[j] + zz[j+1])\n",
    "            solu_t.append(value)\n",
    "        solu.append(solu_t)\n",
    "    return x, t, solu\n",
    "\n",
    "def gen_analytical_cell_averaged(delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq):\n",
    "    x = arange(xmin, xmax+delta_x, delta_x)\n",
    "    t = arange(tmin, tmax+delta_t, delta_t)\n",
    "    solu = []\n",
    "    for ti in range(len(t)):\n",
    "        solu_t = []\n",
    "        for j in range(len(x)-1):\n",
    "            value = integrate.quad(lambda x: analytical_eq(x, t[ti]), x[j], x[j+1])\n",
    "            value = value[0] * (1/delta_x)\n",
    "            solu_t.append(value)\n",
    "        solu.append(solu_t)\n",
    "    return x, t, solu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======\n",
    "# OPTIONS\n",
    "# =======\n",
    "\n",
    "# ===========\n",
    "# training set\n",
    "# ===========\n",
    "\n",
    "def get_trainingset_random(solu, num1, num2, padding, size):\n",
    "    solu_padding = []\n",
    "    pairs = []\n",
    "    for item in solu:\n",
    "        p = padding(item, num1, num2)\n",
    "        solu_padding.append(p)\n",
    "    for iteration in range(size):\n",
    "        t_index = random.randint(0, len(solu_padding)-2)\n",
    "        x_index = random.randint(0, len(p)-num1-num2-1)\n",
    "        time = solu_padding[t_index]\n",
    "        time_next = solu_padding[t_index+1]\n",
    "        train = time[x_index: x_index+num1+num2+1]\n",
    "        target = time_next[x_index+num1]\n",
    "        pair = {'train': train, 'target': target}\n",
    "        pairs.append(pair)\n",
    "    return pairs\n",
    "    \n",
    "def get_trainingset_all(solu, num1, num2, padding):\n",
    "    solu_cutted = solu[:-1]\n",
    "    pairs = []\n",
    "    for index, item in enumerate(solu_cutted):\n",
    "        p = padding(item, num1, num2)\n",
    "        p_next = padding(solu[index+1], num1, num2)\n",
    "        for xi in range(len(p)-num1-num2):\n",
    "            train = p[xi: xi+num1+num2+1]\n",
    "            target = p_next[xi+num1]\n",
    "            pair = {'train': train, 'target': target}\n",
    "            pairs.append(pair)\n",
    "    return pairs\n",
    "\n",
    "# ===========\n",
    "#  testing set\n",
    "# ===========\n",
    "\n",
    "def get_testingset_random(solu, num1, num2, padding, size):\n",
    "    return get_trainingset_random(solu, num1, num2, padding, size)\n",
    "\n",
    "def get_testingset_all(solu, num1, num2, padding):\n",
    "    solu_cutted = solu[:-1]\n",
    "    pairs = []\n",
    "    for index, item in enumerate(solu_cutted):\n",
    "        p = padding(item, num1, num2)\n",
    "        p_next = padding(solu[index+1], num1, num2)\n",
    "        pairs_t = []\n",
    "        for xi in range(len(p)-num1-num2):\n",
    "            train = p[xi:xi+num1+num2+1]\n",
    "            target = p_next[xi+num1]\n",
    "            pair = {'train': train, 'target': target}\n",
    "            pairs_t.append(pair)\n",
    "        pairs.append(pairs_t)\n",
    "    return pairs\n",
    "\n",
    "# x, t, solu = gen_analytical_cell_averaged(delta_x=1/10, delta_t=1/20, xmin=0, tmin=0, xmax=1, tmax=2, analytical_eq=heat_equ_analytical_solu)\n",
    "# pairs = get_testingset_all(solu, 3, 3, recursive_padding)\n",
    "# for i, p in enumerate(pairs[0]):\n",
    "#     print(i, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============\n",
    "# FullconnectedResNet\n",
    "# ==============\n",
    "\n",
    "class FullconnectedResNet(nn.Module):\n",
    "    def __init__(self, i, o, layer_data, hasDropout, p, num, weight=1):\n",
    "        super(FullconnectedResNet, self).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module(\"linear_1\", nn.Linear(i, layer_data[0]))\n",
    "        if hasDropout:\n",
    "            self.layers.add_module(\"dropout_1\", nn.Dropout(p=p))\n",
    "        self.layers.add_module(\"relu_1\", nn.ReLU())\n",
    "        for index in range(len(layer_data)-1):\n",
    "            self.layers.add_module(\"linear_\"+str(index+2), nn.Linear(layer_data[index], layer_data[index+1]))\n",
    "            if hasDropout:\n",
    "                self.layers.add_module(\"dropout_2\", nn.Dropout(p=p))\n",
    "            self.layers.add_module(\"relu_\"+str(index+2), nn.ReLU())\n",
    "        self.layers.add_module(\"linear_3\"+str(len(layer_data)+1), nn.Linear(layer_data[len(layer_data)-1], o))\n",
    "        if hasDropout:\n",
    "            self.layers.add_module(\"dropout_3\", nn.Dropout(p=p))\n",
    "        self.layers.add_module(\"relu_\"+str(len(layer_data)+1), nn.ReLU())\n",
    "        self.num = num\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "        return output + self.weight * x[self.num], x[self.num], output\n",
    "\n",
    "    def load_model(self, save_path):\n",
    "        self.load_state_dict(torch.load(save_path))\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "        torch.save(self.state_dict(), save_path)\n",
    "        \n",
    "# ==========\n",
    "# BidirectionRNN\n",
    "# ==========\n",
    "\n",
    "# class BidirectionRNN(nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printer_of_nn(counter, train, target, loss, output, res, pure, TorE):\n",
    "    print(\"=\"*60)\n",
    "    print(\"This is\", counter+1, \"times of iteration\")\n",
    "    if TorE:\n",
    "        print(\"Training...\")\n",
    "    else:\n",
    "        print(\"Evaluating...\")\n",
    "    print(\"The input is:\", train)\n",
    "    print(\"The output is:\", output)\n",
    "    print(\"The correct solution is:\", target)\n",
    "    print(\"The pure output is:\", pure)\n",
    "    print(\"The res input is:\", res)\n",
    "    if TorE:\n",
    "        print(\"The training loss is:\", loss)\n",
    "    else:\n",
    "        print(\"The evaluation loss is:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_config_generator(\n",
    "    delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq, gen_analytical_method, allTheTime,\n",
    "    num1, num2, padding, train_takeAll, eval_takeAll, size1, size2,\n",
    "    layer_data, nn_model, optim, learning_rate, iteration, hasDropout, p, weight,\n",
    "    loading, load_path,\n",
    "    actual_solu_file, config_file, train_loss_file, model_file, eval_loss_file, prediction_file\n",
    "):\n",
    "    data_setting = {'delta_x': delta_x,'delta_t': delta_t,'xmin': xmin, 'tmin': tmin, 'xmax': xmax, 'tmax': tmax, 'PDE': str(analytical_eq), 'method': str(gen_analytical_method), 'allTheTime': allTheTime}\n",
    "    pairs_setting = {'left': num1,'right': num2, 'dim': num1+num2+1, 'padding': str(padding), 'train_takeAll': train_takeAll, 'eval_takeAll': eval_takeAll, 'size_train': size1, 'size_eval': size2}\n",
    "    nn_setting = {'model': str(nn_model), 'layer_data': layer_data, 'input': num1+num2+1, 'output': 1, 'hasDropout': hasDropout, 'dropout_rate': p, 'res_weight': weight}\n",
    "    optim_setting = {'optimizer': str(optim), 'learning_rate': learning_rate}\n",
    "    model_setting = {'nn_model': nn_setting, 'optim': optim_setting, 'iteration': iteration, 'is_loading': loading, 'loading_path': load_path}\n",
    "    actual_solu_file_setting = {'path': actual_solu_file, 'explain': 'the actual/correct/analytical solutions'}\n",
    "    config_file_setting = {'path': config_file, 'explain': 'the current file, store all the configurations of each experiment'}\n",
    "    train_loss_file_setting = {'path': train_loss_file, 'explain': 'the training loss'}\n",
    "    model_file_setting = {'path': model_file, 'explain': 'the saved model, can be loaded for further training'}\n",
    "    eval_loss_file_setting = {'path': eval_loss_file, 'explain': 'the evaluation loss'}\n",
    "    prediction_file_setting = {'path': prediction_file, 'explain': 'final predictions'}\n",
    "    files = {\n",
    "        'actual_solu_file_setting': actual_solu_file_setting,\n",
    "        'config_file_setting': config_file_setting,\n",
    "        'train_loss_file_setting': train_loss_file_setting,\n",
    "        'model_file_setting': model_file_setting,\n",
    "        'eval_loss_file_setting': eval_loss_file_setting,\n",
    "        'prediction_file_setting': prediction_file_setting\n",
    "    }\n",
    "    json_data = {\n",
    "        'data_setting': data_setting,\n",
    "        'pairs_setting': pairs_setting,\n",
    "        'model_setting': model_setting,\n",
    "        'files_setting': files\n",
    "    }\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_the_model(\n",
    "    folder_name,\n",
    "    delta_x=1/20, delta_t=1/10, xmin=0, tmin=0, xmax=1, tmax=2, analytical_eq=heat_equ_analytical_solu, gen_analytical_method=gen_analytical_cell_averaged, allTheTime=False,\n",
    "    num1=3, num2=3, padding=recursive_padding, train_takeAll=True, eval_takeAll=True, size1=0, size2=0,\n",
    "    layer_data=[6, 6], nn_model=FullconnectedResNet, optim=optim.Adam, learning_rate=0.001, iteration=100, hasDropout=False, p=0.2, weight=1,\n",
    "    loading=False, load_path=\"\",\n",
    "    config_file=\"config.json\", actual_solu_file='actual solution.csv',\n",
    "    train_loss_file='training loss.txt', model_file='model',\n",
    "    eval_loss_file='evaluation loss.txt', prediction_file='prediction.csv',\n",
    "    doEval=True, printTraining=False, printEval=False\n",
    "):\n",
    "    # ====================\n",
    "    # Configuration and parameters\n",
    "    # ====================\n",
    "    json_data = json_config_generator(\n",
    "    delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq, gen_analytical_method, allTheTime,\n",
    "    num1, num2, padding, train_takeAll, eval_takeAll, size1, size2,\n",
    "    layer_data, nn_model, optim, learning_rate, iteration, hasDropout, p, weight,\n",
    "    loading, load_path,\n",
    "    actual_solu_file, config_file, train_loss_file, model_file, eval_loss_file, prediction_file)\n",
    "    # ==================\n",
    "    # Creating pathes and config\n",
    "    # ==================\n",
    "    actual_solu_file = folder_name+\"/\"+actual_solu_file\n",
    "    config_file = folder_name+\"/\"+config_file\n",
    "    train_loss_file = folder_name+\"/\"+train_loss_file\n",
    "    model_file = folder_name+\"/\"+model_file\n",
    "    eval_loss_file = folder_name+\"/\"+eval_loss_file\n",
    "    prediction_file = folder_name+\"/\"+prediction_file\n",
    "    # ==========\n",
    "    # Creating folder\n",
    "    # ==========\n",
    "    mkdir(folder_name)\n",
    "    # ===========\n",
    "    # Prepare the data\n",
    "    # ===========\n",
    "    x, t, solu = gen_analytical_method(delta_x=delta_x, delta_t=delta_t, xmin=xmin, tmin=tmin, xmax=xmax, tmax=tmax, analytical_eq=analytical_eq)\n",
    "    actual_solu = solu\n",
    "    if not allTheTime:\n",
    "        solu = solu[:2]\n",
    "    # ===========\n",
    "    # Prepare the pairs\n",
    "    # ===========\n",
    "    if train_takeAll:\n",
    "        pairs = get_trainingset_all(solu, num1, num2, padding)\n",
    "    else:\n",
    "        pairs = get_trainingset_random(solu, num1, num2, padding, size)\n",
    "    # =================\n",
    "    # Set up model & optimizer\n",
    "    # =================\n",
    "    model = nn_model(i=num1+num2+1, o=1, layer_data=layer_data, hasDropout=hasDropout, p=p, num=num1, weight=weight)\n",
    "    if loading:\n",
    "        model.load_model(load_path)\n",
    "    optimizer = optim(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    # ===========\n",
    "    # Train the model\n",
    "    # ===========\n",
    "    model.train()\n",
    "    list_of_losses_t = []\n",
    "    list_of_outputs = []\n",
    "    counter = 0\n",
    "    for itera in range(iteration):\n",
    "        for pair in pairs:\n",
    "            train = torch.FloatTensor(pair[\"train\"])\n",
    "            target = torch.FloatTensor([pair[\"target\"]])\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            output, res, pure = model(train)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            list_of_losses_t.append(loss.item())\n",
    "            list_of_outputs.append(output)\n",
    "        if printTraining:\n",
    "            printer_of_nn(counter, train, target, loss, output, res, pure, True)\n",
    "        counter += 1\n",
    "    for parameter in model.parameters():\n",
    "        print(parameter)\n",
    "    # =====\n",
    "    # Saving\n",
    "    # =====\n",
    "    save_csv(actual_solu_file, actual_solu)\n",
    "    save_json(config_file, json_data)\n",
    "    save_list(train_loss_file, list_of_losses_t)\n",
    "    model.save_model(model_file)\n",
    "    # ==========\n",
    "    # Do we do eval?\n",
    "    # ==========\n",
    "    if not doEval:\n",
    "        return 0\n",
    "    # ===========\n",
    "    # Prepare the pairs\n",
    "    # ===========\n",
    "    if eval_takeAll:\n",
    "        eval_pairs = get_testingset_all(actual_solu, num1, num2, padding)\n",
    "    else:\n",
    "        eval_pairs = get_testingset_random(actual_solu, num1, num2, padding, size)\n",
    "    # =============\n",
    "    # Evaluate the model\n",
    "    # =============\n",
    "    model.eval()\n",
    "    list_of_losses_e = []\n",
    "    prediction = [solu[0]]\n",
    "    counter = 0\n",
    "    for j in range(len(eval_pairs)):\n",
    "        pairs_t = eval_pairs[j]\n",
    "        prediction_t = []\n",
    "        for pair in pairs_t:\n",
    "            train = torch.FloatTensor(pair[\"train\"])\n",
    "            target = torch.FloatTensor([pair[\"target\"]])\n",
    "            output, res, pure = model(train)\n",
    "            loss = criterion(output, target)\n",
    "            list_of_losses_e.append(loss.item())\n",
    "            prediction_t.append(output.item())\n",
    "        if printTraining:\n",
    "            printer_of_nn(counter, train, target, loss, output, res, pure, False)\n",
    "        counter += 1\n",
    "        prediction.append(prediction_t)\n",
    "    # =====\n",
    "    # Saving\n",
    "    # =====\n",
    "    save_list(eval_loss_file, list_of_losses_e)\n",
    "    save_csv(prediction_file, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "This is 1 times of iteration\n",
      "Training...\n",
      "The input is: tensor([-0.8873, -0.7042, -0.4521, -0.1558,  0.1558,  0.4521,  0.7042])\n",
      "The output is: tensor([0.1147], grad_fn=<AddBackward0>)\n",
      "The correct solution is: tensor([-0.0951])\n",
      "The pure output is: tensor([0.2705], grad_fn=<ReluBackward0>)\n",
      "The res input is: tensor(-0.1558)\n",
      "The training loss is: tensor(0.0440, grad_fn=<MseLossBackward>)\n",
      "Parameter containing:\n",
      "tensor([[-0.2973, -0.1087, -0.3091, -0.0948,  0.3147, -0.2510, -0.1680],\n",
      "        [-0.1857, -0.1098,  0.0387, -0.0539, -0.0397,  0.0245,  0.1705],\n",
      "        [ 0.2237, -0.1473,  0.2685, -0.0444, -0.1420,  0.1728,  0.2436],\n",
      "        [ 0.0600, -0.0121, -0.2992, -0.0904,  0.0619,  0.3543,  0.2189],\n",
      "        [-0.1779, -0.3338, -0.3495,  0.3396, -0.2163, -0.3530, -0.1972],\n",
      "        [ 0.3253,  0.1738, -0.2024,  0.3199, -0.3229,  0.0014, -0.1849]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0529,  0.1445, -0.0480,  0.1122, -0.0805,  0.3381],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3545,  0.3425,  0.2806, -0.3508,  0.2071,  0.0421],\n",
      "        [-0.3612,  0.3768,  0.3238,  0.3174, -0.3074,  0.3417],\n",
      "        [-0.1952,  0.3233, -0.3721, -0.2497, -0.2858, -0.1634],\n",
      "        [-0.0695,  0.2588, -0.1602, -0.1977,  0.2411,  0.0900],\n",
      "        [-0.0027, -0.2849, -0.3820, -0.0556,  0.1620,  0.1271],\n",
      "        [ 0.3906,  0.3958, -0.0296,  0.2217, -0.1047, -0.3429]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 2.3566e-01,  3.9362e-01, -1.8792e-01, -2.4165e-01,  1.4054e-01,\n",
      "         2.2401e-04], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0363,  0.0673, -0.3291, -0.3515, -0.0807,  0.3064]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0872], requires_grad=True)\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '10 10 control group/actual solution.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-344f2de448de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m training_the_model(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mdelta_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     folder_name=\"10 10 control group\", iteration=1, printTraining=True, printEval=True)\n",
      "\u001b[1;32m<ipython-input-58-42b13a173161>\u001b[0m in \u001b[0;36mtraining_the_model\u001b[1;34m(folder_name, delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq, gen_analytical_method, allTheTime, num1, num2, padding, train_takeAll, eval_takeAll, size1, size2, layer_data, nn_model, optim, learning_rate, iteration, hasDropout, p, weight, loading, load_path, config_file, actual_solu_file, train_loss_file, model_file, eval_loss_file, prediction_file, doEval, printTraining, printEval)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# Saving\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# =====\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0msave_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_solu_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual_solu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[0msave_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0msave_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_of_losses_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ab85d479ee2c>\u001b[0m in \u001b[0;36msave_csv\u001b[1;34m(save_path, data)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msave_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w+\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '10 10 control group/actual solution.csv'"
     ]
    }
   ],
   "source": [
    "training_the_model(\n",
    "    delta_x=1/10, delta_t=1/20, xmin=0, tmin=0, xmax=2, tmax=1, learning_rate=0.001,\n",
    "    folder_name=\"10 10 control group\", iteration=1, printTraining=True, printEval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
