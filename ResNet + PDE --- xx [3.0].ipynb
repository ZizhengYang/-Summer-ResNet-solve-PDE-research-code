{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate as integrate\n",
    "import scipy.special as special\n",
    "\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim\n",
    "\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import exp,arange\n",
    "from pylab import meshgrid,cm,imshow,contour,clabel,colorbar,axis,title,show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sin(\\pi x) e^{-\\pi^2 t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_equ_analytical_solu(x, t):\n",
    "    return np.sin(np.pi * x) * np.exp(-np.power(np.pi, 2) * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============\n",
    "# save and load: json\n",
    "# ============\n",
    "\n",
    "def save_json(save_path, data):\n",
    "    assert save_path.split('.')[-1] == 'json'\n",
    "    with open(save_path,'w+') as file:\n",
    "        json.dump(data,file)\n",
    "\n",
    "def load_json(file_path):\n",
    "    assert file_path.split('.')[-1] == 'json'\n",
    "    with open(file_path,'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# ============\n",
    "# save and load: csv\n",
    "# ============\n",
    "\n",
    "def save_csv(save_path, data):\n",
    "    with open(save_path, \"w+\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)\n",
    "\n",
    "def load_csv(file_path):\n",
    "    string = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            if i%2==0:\n",
    "                string.append(row)\n",
    "    ret = []\n",
    "    for p in string:\n",
    "        ret.append(np.array([float(i) for i in p]))\n",
    "    return ret\n",
    "    \n",
    "# ==============\n",
    "# save and load: normal\n",
    "# ==============\n",
    "\n",
    "def save_list(save_path, data):\n",
    "    file = open(save_path, 'w+')\n",
    "    for value in data:\n",
    "        file.write(str(value)+\" \")\n",
    "    file.close()\n",
    "\n",
    "# def load_list(file_path):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======\n",
    "# OPTIONS\n",
    "# =======\n",
    "\n",
    "# ===========\n",
    "# padding method\n",
    "# ===========\n",
    "\n",
    "def padding(starting_padding, original, end_padding):\n",
    "    return np.hstack((starting_padding, original, end_padding)).tolist()\n",
    "\n",
    "def zero_padding(original, num1, num2):\n",
    "    starting_padding = [0 for i in range(num1)]\n",
    "    end_padding = [0 for i in range(num2)]\n",
    "    return padding(starting_padding, original, end_padding)\n",
    "\n",
    "def border_padding(original, num1, num2):\n",
    "    starting = original[0]\n",
    "    ending = original[len(original)-1]\n",
    "    starting_padding = [starting for i in range(num1)]\n",
    "    end_padding = [ending for i in range(num2)]\n",
    "    return padding(starting_padding, original, end_padding)\n",
    "\n",
    "def recursive_padding(original, num1, num2):\n",
    "    starting_padding = original[-num1:]\n",
    "    end_padding = original[:num2]\n",
    "    return padding(starting_padding, original, end_padding)\n",
    "\n",
    "def random_padding(original, num1, num2):\n",
    "    max_value = np.max(original)\n",
    "    min_value = np.min(original)\n",
    "    starting_padding = [random.randint(min_value, max_value) for i in range(num1)]\n",
    "    end_padding = [random.randint(min_value, max_value) for i in range(num2)]\n",
    "    return padding(starting_padding, original, end_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======\n",
    "# OPTIONS\n",
    "# =======\n",
    "\n",
    "# ==================\n",
    "# analytical solution generator\n",
    "# ==================\n",
    "\n",
    "def gen_analytical(delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq):\n",
    "    x = arange(xmin, xmax, delta_x)\n",
    "    t = arange(tmin, tmax, delta_t)\n",
    "    X,T = meshgrid(x, t) # grid of point\n",
    "    solu = analytical_eq(X, T) # evaluation of the function on the grid\n",
    "    return x, t, solu\n",
    "\n",
    "def gen_analytical_averaged(delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq):\n",
    "    x = arange(xmin, xmax, delta_x)\n",
    "    t = arange(tmin, tmax, delta_t)\n",
    "    X,T = meshgrid(x, t) # grid of point\n",
    "    Z = analytical_eq(X, T) # evaluation of the function on the grid\n",
    "    solu = []\n",
    "    for zz in Z:\n",
    "        solu_t = []\n",
    "        for j in range(len(zz)-1):\n",
    "            value = (1/2) * (zz[j] + zz[j+1])\n",
    "            solu_t.append(value)\n",
    "        solu.append(solu_t)\n",
    "    return x, t, solu\n",
    "\n",
    "def gen_analytical_cell_averaged(delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq):\n",
    "    x = arange(xmin, xmax, delta_x)\n",
    "    t = arange(tmin, tmax, delta_t)\n",
    "    solu = []\n",
    "    for ti in range(len(t)):\n",
    "        solu_t = []\n",
    "        for j in range(len(x)-1):\n",
    "            value = integrate.quad(lambda x: analytical_eq(x, t[ti]), x[j], x[j+1])\n",
    "            value = value[0] * (1/delta_x)\n",
    "            solu_t.append(value)\n",
    "        solu.append(solu_t)\n",
    "    return x, t, solu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======\n",
    "# OPTIONS\n",
    "# =======\n",
    "\n",
    "# ===========\n",
    "# training set\n",
    "# ===========\n",
    "\n",
    "def get_trainingset_random(solu, num1, num2, padding, size):\n",
    "    solu_padding = []\n",
    "    pairs = []\n",
    "    for item in solu:\n",
    "        p = padding(item, num1, num2)\n",
    "        solu_padding.append(p)\n",
    "    for iteration in range(size):\n",
    "        t_index = random.randint(0, len(solu_padding)-2)\n",
    "        x_index = random.randint(0, len(p)-num1-num2-1)\n",
    "        time = solu_padding[t_index]\n",
    "        time_next = solu_padding[t_index+1]\n",
    "        train = time[x_index: x_index+num1+num2+1]\n",
    "        target = time_next[x_index+num1]\n",
    "        pair = {'train': train, 'target': target}\n",
    "        pairs.append(pair)\n",
    "    return pairs\n",
    "    \n",
    "def get_trainingset_all(solu, num1, num2, padding):\n",
    "    solu_cutted = solu[:-1]\n",
    "    pairs = []\n",
    "    for index, item in enumerate(solu_cutted):\n",
    "        p = padding(item, num1, num2)\n",
    "        p_next = padding(solu[index+1], num1, num2)\n",
    "        for xi in range(len(p)-num1-num2-1):\n",
    "            train = p[xi: xi+num1+num2+1]\n",
    "            target = p_next[xi+num1]\n",
    "            pair = {'train': train, 'target': target}\n",
    "            pairs.append(pair)\n",
    "    return pairs\n",
    "\n",
    "# ===========\n",
    "#  testing set\n",
    "# ===========\n",
    "\n",
    "def get_testingset_random(solu, num1, num2, padding, size):\n",
    "    return get_trainingset_random(solu, num1, num2, padding, size)\n",
    "\n",
    "def get_testingset_all(solu, num1, num2, padding):\n",
    "    solu_cutted = solu[:-1]\n",
    "    pairs = []\n",
    "    for index, item in enumerate(solu_cutted):\n",
    "        p = padding(item, num1, num2)\n",
    "        p_next = padding(solu[index+1], num1, num2)\n",
    "        pairs_t = []\n",
    "        for xi in range(len(p)-num1-num2-1):\n",
    "            train = p[xi:xi+num1+num2+1]\n",
    "            target = p_next[xi+num1]\n",
    "            pair = {'train': train, 'target': target}\n",
    "            pairs_t.append(pair)\n",
    "        pairs.append(pairs_t)\n",
    "    return pairs\n",
    "\n",
    "# x, t, solu = gen_analytical_cell_averaged(delta_x=1/20, delta_t=1/10, xmin=0, tmin=0, xmax=2, tmax=1, analytical_eq=heat_equ_analytical_solu)\n",
    "# pairs = get_testingset_all(solu, 3, 3, recursive_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============\n",
    "# FullconnectedResNet\n",
    "# ==============\n",
    "\n",
    "class FullconnectedResNet(nn.Module):\n",
    "    def __init__(self, weight=1, i, o, layer_data, hasDropout, p, num):\n",
    "        super(FullconnectedResNet, self).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module(\"linear_1\", nn.Linear(i, layer_data[0]))\n",
    "        if hasDropout:\n",
    "            self.layers.add_module(\"dropout_1\", nn.Dropout(p=p))\n",
    "        self.layers.add_module(\"relu_1\", nn.ReLU())\n",
    "        for index in range(len(layer_data)-1):\n",
    "            self.layers.add_module(\"linear_\"+str(index+2), nn.Linear(layer_data[index], layer_data[index+1]))\n",
    "            if hasDropout:\n",
    "                self.layers.add_module(\"dropout_2\", nn.Dropout(p=p))\n",
    "            self.layers.add_module(\"relu_\"+str(index+2), nn.ReLU())\n",
    "        self.layers.add_module(\"linear_3\"+str(len(layer_data)+1), nn.Linear(layer_data[len(layer_data)-1], o))\n",
    "        if hasDropout:\n",
    "            self.layers.add_module(\"dropout_3\", nn.Dropout(p=p))\n",
    "        self.layers.add_module(\"relu_\"+str(len(layer_data)+1), nn.ReLU())\n",
    "        self.num = num\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "        return output + self.weight * x[self.num]\n",
    "\n",
    "    def load_model(self, save_path):\n",
    "        self.load_state_dict(torch.load(save_path))\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "        torch.save(self.state_dict(), save_path)\n",
    "        \n",
    "# ==========\n",
    "# BidirectionRNN\n",
    "# ==========\n",
    "\n",
    "# class BidirectionRNN(nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printer_of_nn(train, target, loss, output):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_config_generator(\n",
    "    delta_x, delta_t, xmin, tmin, xmax, tmax, analytical_eq, gen_analytical_method, allTheTime\n",
    "    num1, num2, padding, train_takeAll, eval_takeAll, size1, size2,\n",
    "    layer_data, nn_model, optim, learning_rate, iteration, hasDropout, p, weight,\n",
    "    loading, load_path,\n",
    "    config_file):\n",
    "    data_setting = {'delta_x': delta_x,'delta_t': delta_t,'xmin': xmin, 'tmin': tmin, 'xmax': xmax, 'tmax': tmax, 'PDE': str(analytical_eq), 'method': str(gen_analytical_method), 'allTheTime': allTheTime}\n",
    "    pairs_setting = {'left': num1,'right': num2, 'dim': num1+num2+1, 'padding': str(padding), 'train_takeAll': train_takeAll, 'eval_takeAll': eval_takeAll, 'size_train': size1, 'size_eval': size2}\n",
    "    nn_setting = {'model': str(nn_model), 'layer_data': layer_data, 'input': num1+num2+1, 'output': 1, 'hasDropout': hasDropout, 'dropout_rate': p, 'res_weight': weight}\n",
    "    optim_setting = {'optimizer': str(optim), 'learning_rate': learning_rate}\n",
    "    model_setting = {'nn_model': nn_setting, 'optim': optim_setting, 'iteration': iteration, 'is_loading': loading, 'loading_path': load_path}\n",
    "    json_data = {\n",
    "        'data_setting': data_setting,\n",
    "        'pairs_setting': pairs_setting,\n",
    "        'model_setting': model_setting\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_the_model(\n",
    "    delta_x=1/20, delta_t=1/10, xmin=0, tmin=0, xmax=2, tmax=1, analytical_eq=heat_equ_analytical_solu, gen_analytical_method=gen_analytical_cell_averaged, allTheTime=False,\n",
    "    num1=3, num2=3, padding=recursive_padding, train_takeAll=True, eval_takeAll=True, size1=0, size2=0,\n",
    "    layer_data=[6, 6], nn_model=FullconnectedResNet, optim=optim.Adam, learning_rate=0.001, iteration=100, hasDropout=False, p=0.2, weight=1,\n",
    "    loading=False, load_path=\"\",\n",
    "    folder=\"\", config_file=\"\", actaul_solu_file=\"\",\n",
    "    doEval=True, printTraining=False, printEval=False):\n",
    "    # ===========\n",
    "    # Prepare the data\n",
    "    # ===========\n",
    "    x, t, solu = gen_analytical_method(delta_x=delta_x, delta_t=delta_t, xmin=xmin, tmin=tmin, xmax=xmax, tmax=tmax, analytical_eq=analytical_eq)\n",
    "    actual_solu = solu\n",
    "    if not allTheTime:\n",
    "        solu = solu[:2]\n",
    "    # ===========\n",
    "    # Prepare the pairs\n",
    "    # ===========\n",
    "    if train_takeAll:\n",
    "        pairs = get_trainingset_all(solu, num1, num2, padding)\n",
    "    else:\n",
    "        pairs = get_trainingset_random(solu, num1, num2, padding, size)\n",
    "    # =================\n",
    "    # Set up model & optimizer\n",
    "    # =================\n",
    "    model = nn_model(weight=weight, i=num1+num2+1, o=1, layer_data=layer_data, hasDropout=hasDropout, p=p, num=num1)\n",
    "    optimizer = optim(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    # ===========\n",
    "    # Train the model\n",
    "    # ===========\n",
    "    model.train()\n",
    "    list_of_losses_t = []\n",
    "    list_of_outputs = []\n",
    "    counter = 0\n",
    "    for itera in range(iteration):\n",
    "        for pair in pairs:\n",
    "            train = torch.FloatTensor(pair[\"train\"])\n",
    "            target = torch.FloatTensor([pair[\"target\"]])\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            list_of_losses_t.append(loss.item())\n",
    "            list_of_output.append(output)\n",
    "            if printTraining:\n",
    "                printer_of_nn(train, target, loss, output)\n",
    "            counter += 1\n",
    "    # ===========\n",
    "    # Prepare the pairs\n",
    "    # ===========\n",
    "    if eval_takeAll:\n",
    "        eval_pairs = get_testingset_all(solu, num1, num2, padding)\n",
    "    else:\n",
    "        eval_pairs = get_testingset_random(solu, num1, num2, padding, size)\n",
    "    # =============\n",
    "    # Evaluate the model\n",
    "    # =============\n",
    "    model.eval()\n",
    "    list_of_losses_e = []\n",
    "    prediction = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file=\"config.json\", solu_file=\"solu.csv\", input_file=\"inputs.txt\", loss_file=\"train_losses\", outout_file=\"outputs.txt\", model_file = \"model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
